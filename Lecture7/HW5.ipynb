{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["bjjkmbQh5UXi","V2RUQR5Z5qY6","m7jaI_RGlyQb","ctVw108k6pL1","4BojYoiu65iK","sfok6kRznXin","vI1jvfz6pcq6","XD_kocPQpp93","SZzQgj_cq2Eh","g1vZgsuCjQmt","_jXVN8y99n16","jGBpKqcnpOZo","Jz1CdzD29XUR"],"authorship_tag":"ABX9TyN9YH/F6UeoSlhpRLl3NA4E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üî¢ HW 5: Learning to Integrate Using Transformers\n","\n","<font color='red'>**Deadline: 22.05.2024**</font>\n","\n","In this notebook, we explore the task of **symbolic integration** using transformer-based models. We will be working with a pretrained transformer developed by Facebook AI Research (Meta AI) that was specifically designed for mathematical tasks.\n","\n","As discussed previously, training such models from scratch is computationally expensive. Therefore, we will use an existing pretrained model and demonstrate how it can be used to perform integration tasks.\n","\n"],"metadata":{"id":"PMovBfzBZ1ur"}},{"cell_type":"markdown","source":["## üì¶ Step 1: Clone the Repository and Import Transformer Components\n"],"metadata":{"id":"bjjkmbQh5UXi"}},{"cell_type":"markdown","source":["We begin by cloning the GitHub repository that contains the necessary model definitions, pretrained weights, and utility scripts for working with mathematical problems.\n","\n","```python\n","!git clone https://github.com/facebookresearch/SymbolicMathematics\n","```"],"metadata":{"id":"L2Iw2Ekx5biG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xE-43drwZHmM"},"outputs":[],"source":["!git clone https://github.com/facebookresearch/SymbolicMathematics.git"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import sympy as sp\n","import torch\n","from logging import getLogger"],"metadata":{"id":"l4z70Hm4aROm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have the repository cloned, we‚Äôll load the core transformer building blocks and utility functions. Although the implementation may differ slightly from our lecture examples, the overall API and workflow remain the same.\n"],"metadata":{"id":"RFyMeh3U5gD_"}},{"cell_type":"code","source":["from SymbolicMathematics.src.model import build_modules\n","from SymbolicMathematics.src.utils import to_cuda\n","from SymbolicMathematics.src.envs.sympy_utils import simplify\n","from SymbolicMathematics.src.model.transformer import TransformerModel\n","\n","TransformerModel.STORE_OUTPUTS = True"],"metadata":{"id":"DmJN5xH1jHdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üßÆ Step 2: Dataset Generation"],"metadata":{"id":"V2RUQR5Z5qY6"}},{"cell_type":"markdown","source":["To train and evaluate the transformer on symbolic integration tasks, the authors of the paper proposed **three distinct methods** for constructing function-integral pairs:\n","\n","### üîÅ 1. Forward Generation (FWD)\n","- Randomly generate a function.\n","- Use a **Computer Algebra System (CAS)** to compute its integral.\n","- If the CAS cannot compute the integral, discard the pair.\n","- ‚úÖ Ensures the result is a correct integral.\n","- ‚ùå May lead to a biased dataset, as only \"easy-to-integrate\" functions are retained.\n","\n","### üîÑ 2. Backward Generation (BWD)\n","- Randomly generate a function **\\( F \\)** (that has known structure).\n","- Compute its **derivative** \\( f = F' \\).\n","- Use the pair \\( (f, F) \\) as a training sample.\n","- ‚úÖ Guarantees correctness.\n","- ‚ùå Trains the model to \"undo differentiation\", which is not the same as general integration.\n","\n","### üß© 3. Backward Generation with Integration by Parts (IBP)\n","- Generate two random functions \\( F \\) and \\( G \\).\n","- Compute their derivatives \\( f = F',\\ g = G' \\).\n","- If either \\( f \\cdot G \\) or \\( F \\cdot g \\) is already in the dataset, apply:\n","  \\[\n","  \\int Fg = FG - \\int fG\n","  \\]\n","- ‚úÖ Encourages more diverse integration strategies (like substitution or parts).\n","- ‚úÖ Mimics human mathematical reasoning.\n","\n","---\n","\n","### üìå Next Step: Choose a Model\n","\n","We will now select one of the pretrained transformer models for evaluation and testing on integration problems.\n","\n","\n","<div align=\"center\">\n","  <img src=\"https://drive.google.com/uc?id=1wK3J_CPmJkRlYyWK0qojAwCSEF4XmcPv\" alt=\"Integration Transformer Pipeline\" width=\"600\"/>\n","</div>"],"metadata":{"id":"f8x9OShLko44"}},{"cell_type":"markdown","source":["Model links in that order:\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/bwd.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ibp.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ibp_bwd.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd_ibp.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ode1.pth\n","\n","https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ode2.pth"],"metadata":{"id":"2g1ojChYk0aM"}},{"cell_type":"markdown","source":["To use a pretrained transformer model for integration, we download the weights for the **FWD+BWD** model ‚Äî a combination of forward and backward generation strategies.\n","\n","This model has demonstrated strong performance in generalizing across a wide range of symbolic integration tasks.\n","\n","We will store the weights in a path that we can later pass to the model loader."],"metadata":{"id":"zuFFpC-NldrU"}},{"cell_type":"code","source":["# Download the pretrained weights (FWD+BWD model)\n","!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth -P pretrained_models/"],"metadata":{"id":"6yUU53ogj4g9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the path to the downloaded weights\n","MODEL_PATH = \"pretrained_models/fwd_bwd.pth\""],"metadata":{"id":"CgqerxUxlhQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üß† Step 3: Load the Transformer Model\n","\n","We now define the configuration for our transformer model. These parameters are taken directly from the official [SymbolicMathematics GitHub repository](https://github.com/facebookresearch/SymbolicMathematics/blob/main/README.md).\n","\n","\n","We also specify that the model will run on **CPU** for compatibility, but you may switch to CUDA if available by setting `'cpu': False`.\n","\n","The configuration is wrapped using the `AttrDict` class, which enables attribute-style access."],"metadata":{"id":"m7jaI_RGlyQb"}},{"cell_type":"code","source":["from SymbolicMathematics.src.utils import AttrDict\n","\n","model_path = MODEL_PATH  # Make sure this path points to your pretrained weights\n","\n","params = AttrDict({\n","    # environment parameters\n","    'env_name': 'char_sp',\n","    'int_base': 10,\n","    'balanced': False,\n","    'positive': True,\n","    'precision': 10,\n","    'n_variables': 1,\n","    'n_coefficients': 0,\n","    'leaf_probs': '0.75,0,0.25,0',\n","    'max_len': 512,\n","    'max_int': 5,\n","    'max_ops': 15,\n","    'max_ops_G': 15,\n","    'clean_prefix_expr': True,\n","    'rewrite_functions': '',\n","    'tasks': 'prim_fwd',\n","    'operators': 'add:10,sub:3,mul:10,div:5,sqrt:4,pow2:4,pow3:2,pow4:1,pow5:1,ln:4,exp:4,sin:4,cos:4,tan:4,asin:1,acos:1,atan:1,sinh:1,cosh:1,tanh:1,asinh:1,acosh:1,atanh:1',\n","\n","    # model parameters\n","    'cpu': True,  # Set to False if using CUDA\n","    'emb_dim': 1024,\n","    'n_enc_layers': 6,\n","    'n_dec_layers': 6,\n","    'n_heads': 8,\n","    'dropout': 0,\n","    'attention_dropout': 0,\n","    'sinusoidal_embeddings': False,\n","    'share_inout_emb': True,\n","    'reload_model': model_path,\n","})"],"metadata":{"id":"u4wRNN0pliG0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We now load the pretrained transformer model using code from the official [SymbolicMathematics repository](https://github.com/facebookresearch/SymbolicMathematics/tree/main).\n","\n","To support running on CPU (and optionally on GPU), we slightly modify the original `build_modules` function from [`src/model/init.py`](https://github.com/facebookresearch/SymbolicMathematics/blob/main/src/model/init.py) by explicitly setting the device based on availability and user preference.\n","\n","This function:\n","- Initializes both **encoder** and **decoder** transformer modules.\n","- Loads pretrained weights from `params.reload_model`.\n","- Automatically removes the `'module.'` prefix if weights were saved with `nn.DataParallel`.\n","\n"],"metadata":{"id":"U2Sx1xOYmAJJ"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() and not params.cpu else \"cpu\")\n","\n","def build_modules_cpu(env, params):\n","    \"\"\"\n","    Load pretrained encoder and decoder modules with CPU/GPU compatibility.\n","    \"\"\"\n","    logger = getLogger()\n","\n","    # Instantiate encoder and decoder\n","    modules = {\n","        'encoder': TransformerModel(params, env.id2word, is_encoder=True, with_output=False),\n","        'decoder': TransformerModel(params, env.id2word, is_encoder=False, with_output=True),\n","    }\n","\n","    # Load pretrained weights\n","    if params.reload_model != '':\n","        logger.info(f\"Reloading modules from {params.reload_model} ...\")\n","        reloaded = torch.load(params.reload_model, map_location=torch.device('cpu'))\n","        for name, model in modules.items():\n","            assert name in reloaded\n","            state_dict = reloaded[name]\n","            if all(k.startswith('module.') for k in state_dict):\n","                state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n","            model.load_state_dict(state_dict)\n","\n","    # Log parameter count\n","    for name, model in modules.items():\n","        logger.info(f\"Number of parameters ({name}): {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n","\n","    # Move to CUDA if available and not forced to use CPU\n","    if not params.cpu:\n","        for model in modules.values():\n","            model.cuda()\n","\n","    return modules"],"metadata":{"id":"ozqzZrQilxEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from SymbolicMathematics.src.envs import build_env\n","\n","env = build_env(params)\n","x = env.local_dict['x']\n","\n","if device == \"cuda\":\n","  print(\"CUDA\")\n","  modules = build_modules(env, params)\n","else:\n","  print(\"CPU\")\n","  modules = build_modules_cpu(env, params)\n","encoder = modules['encoder']\n","decoder = modules['decoder']"],"metadata":{"id":"jTGWfYodmM9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üéØ Problem 1: Analyze Encoder and Decoder Structure (1 Point)"],"metadata":{"id":"ctVw108k6pL1"}},{"cell_type":"markdown","source":["In this task, you will inspect the structure of the loaded Transformer **encoder** and **decoder**.\n","\n","Your goals:\n","1. **Determine the number of attention heads**.\n","2. **Determine the number of attention layers**.\n","3. **Determine the total embedding dimension** used by the model.\n","\n","> üí° Recall:\n","> - The total embedding dimension is split across multiple attention heads.\n","> - You should **verify your answers using actual model parameters**, not just from the `params` dictionary.\n","\n","### ‚úÖ What to Do\n","\n","- Use Python to explore the `encoder` and `decoder` objects.\n","- Print relevant layer configurations.\n","- Make sure to verify and **justify** your answers with code and comments.\n","\n","### üìå Output Format\n","\n","- **Number of attention heads:** `...`\n","- **Number of attention layers:** `...`\n","- **Total embedding dimension:** `...`\n","\n","Each attention head has embedding size: `total_emb_dim / num_heads`\n"],"metadata":{"id":"b8DSmej7mo6m"}},{"cell_type":"code","source":[],"metadata":{"id":"YmHDKgg-6_3W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üß™ Problem 2 (4 points total)"],"metadata":{"id":"4BojYoiu65iK"}},{"cell_type":"markdown","source":["### üß™ Problem 2a: Explore SymPy & Tokenization\n","\n","In this part, you will get familiar with how the symbolic math library **SymPy** handles expressions, and how the **SymbolicMathematics environment** tokenizes them into prefix notation.\n","\n","We define a symbolic function \\( F \\), compute its derivative \\( f = F' \\), and then convert both expressions to prefix form using the environment's `sympy_to_prefix()` method.\n","\n","> üîç Your task is to experiment by modifying `F_infix` and observe how SymPy and the environment behave. Try functions with:\n","> - Basic operations (e.g. `x**2 + 3*x`)\n","> - Trigonometric functions\n","> - Exponentials and logs\n","> - Nested function compositions\n","\n","No submission is needed ‚Äî just use this to make sure the code flow makes sense before we move on to prediction and evaluation."],"metadata":{"id":"sfok6kRznXin"}},{"cell_type":"code","source":["from IPython.display import display\n","import sympy as sp\n","\n","F_infix = '1/cos(x)'\n","# F_infix = 'x * cos(x**2) * tan(x)'\n","# F_infix = 'ln(cos(x + exp(x)) * sin(x**2 + 2) * exp(x) / x)'\n","# F_infix = 'cos(x**2 * exp(x * cos(x)))'\n","# F_infix = 'x**2 + x '\n","# F_infix = '123 * exp(2*x)'\n","# F_infix = '1'\n","\n","# Parse and differentiate\n","F = sp.S(F_infix, locals=env.local_dict)\n","f = F.diff()\n","\n","\n","# Convert to prefix notation\n","F_prefix = env.sympy_to_prefix(F)\n","f_prefix = env.sympy_to_prefix(f)\n","\n","# Pretty output\n","print(\"Original infix string:\")\n","print(F_infix)\n","\n","print(\"\\nParsed SymPy expression for F:\")\n","display(F)\n","\n","print(\"\\nSymbolic derivative f = F':\")\n","display(f)\n","\n","print(\"\\nPrefix form of F:\")\n","print(F_prefix)\n","\n","print(\"\\nPrefix form of f:\")\n","print(f_prefix)"],"metadata":{"id":"jjE2zkyinV8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß™ Problem 2b: Run the Model to Predict the Integral\n","\n","Now that we have the function \\( f = F' \\), we ask the model to predict the original \\( F \\) given only \\( f \\).\n","\n","To do this, we:\n","\n","1. Create a **prefix-encoded input** to represent the symbolic query ‚Äúwhat function has derivative \\( f \\)?‚Äù\n","2. Encode the input using the **encoder** module.\n","3. Use **beam search decoding** on the decoder to generate candidate antiderivatives."],"metadata":{"id":"vI1jvfz6pcq6"}},{"cell_type":"code","source":["\n","\n","x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + f_prefix)\n","\n","# Tokenize and encode the input sequence\n","x1 = torch.LongTensor(\n","    [env.eos_index] +\n","    [env.word2id[w] for w in x1_prefix] +\n","    [env.eos_index]\n",").view(-1, 1)\n","\n","len1 = torch.LongTensor([len(x1)])\n","\n","# If using GPU: uncomment the following\n","# x1, len1 = to_cuda(x1, len1)"],"metadata":{"id":"848fu8rBntn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We pass the symbolic input through the **encoder** to produce a representation of the function \\( f \\)."],"metadata":{"id":"VvIklUsnpiMG"}},{"cell_type":"code","source":["# Encode the input with the Transformer encoder\n","with torch.no_grad():\n","    encoded = encoder('fwd', x=x1, lengths=len1, causal=False).transpose(0, 1)"],"metadata":{"id":"ry_oZiXZpf3-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use **beam search** to generate multiple likely hypotheses for the original function \\( F \\).\n","\n","#### üß† What is Beam Search?\n","\n","Beam search keeps track of the top \\( k \\) most likely outputs (here, `beam_size = 10`) at every decoding step instead of just one. This allows the model to explore multiple plausible symbolic expressions, improving accuracy in structured tasks like integration."],"metadata":{"id":"XD_kocPQpp93"}},{"cell_type":"code","source":["beam_size = 10\n","\n","with torch.no_grad():\n","    _, _, beam = decoder.generate_beam(\n","        encoded,\n","        len1,\n","        beam_size=beam_size,\n","        length_penalty=1.0,\n","        early_stopping=1,\n","        max_len=200\n","    )\n","\n","# Check the number of hypotheses generated\n","assert len(beam) == 1\n","hypotheses = beam[0].hyp\n","assert len(hypotheses) == beam_size"],"metadata":{"id":"67rfn030pmSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we:\n","- Convert each predicted prefix expression into infix and then to a SymPy object.\n","- Symbolically differentiate the predicted expression.\n","- Compare it to the original function \\( f \\)."],"metadata":{"id":"l9HIIe2UpxMP"}},{"cell_type":"code","source":["from sympy import simplify\n","from IPython.display import display, Math\n","\n","def display_model_predictions(f, F, hypotheses, env):\n","    \"\"\"\n","    Display model predictions in a formatted mathematical style.\n","\n","    Parameters:\n","    - f: SymPy expression (the derivative)\n","    - F: SymPy expression (the reference antiderivative)\n","    - hypotheses: list of (score, tensor) pairs returned by beam search\n","    - env: SymbolicMathematics environment, providing prefix <-> infix conversions\n","    \"\"\"\n","    print(\"üî¢ Input function (f):\")\n","    display(f)\n","\n","    print(\"\\nüìå Reference antiderivative (F):\")\n","    display(F)\n","\n","    print(\"\\nüîç Model Predictions:\")\n","\n","    for score, sent in sorted(hypotheses, key=lambda x: x[0], reverse=True):\n","        ids = sent[1:].tolist()\n","        tok = [env.id2word[wid] for wid in ids]\n","\n","        try:\n","            hyp = env.prefix_to_infix(tok)\n","            hyp_sympy = env.infix_to_sympy(hyp)\n","\n","            var = list(f.free_symbols)[0]\n","            res = \"OK\" if simplify(hyp_sympy.diff(var) - f, seconds=1) == 0 else \"NO\"\n","\n","            label = f\"{score:.5f}  {res}\"\n","            display(Math(rf\"\\text{{{label}}} \\quad \\Rightarrow \\quad {sp.latex(hyp_sympy)}\"))\n","\n","        except Exception:\n","            label = f\"{score:.5f}  INVALID PREFIX EXPRESSION\"\n","            display(Math(rf\"\\text{{{label}}} \\quad \\Rightarrow \\quad \\text{{{tok}}}\"))"],"metadata":{"id":"LRPgVU3PpupX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_model_predictions(f, F, hypotheses, env)"],"metadata":{"id":"wNcjPU_npzS_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üß© Wrapping Up the Prediction Process\n","\n","To streamline the workflow, we now wrap the entire prediction pipeline ‚Äî from parsing the function \\( F \\), computing its derivative \\( f \\), preparing the input, encoding it, and generating outputs via beam search ‚Äî into a single helper function.\n","\n","This will make it easier to test different functions with minimal boilerplate, while still giving access to all intermediate results like \\( f \\), \\( F \\), and the list of candidate hypotheses."],"metadata":{"id":"SZzQgj_cq2Eh"}},{"cell_type":"code","source":["def predict_antiderivatives(F_infix, env, encoder, decoder, beam_size=10):\n","    \"\"\"\n","    Given an infix expression for F, compute its derivative f,\n","    generate prefix input for the model, run beam search and return all results.\n","\n","    Returns:\n","        f: sympy expression (the derivative)\n","        F: sympy expression (the reference antiderivative)\n","        hypotheses: list of (score, decoded_tensor) from beam search\n","    \"\"\"\n","    import sympy as sp\n","    import torch\n","\n","    # Parse the input expression\n","    F = sp.S(F_infix, locals=env.local_dict)\n","    var = list(F.free_symbols)[0]\n","    f = F.diff(var)\n","\n","    # Convert derivative to prefix and prepare model input\n","    f_prefix = env.sympy_to_prefix(f)\n","    x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + f_prefix)\n","    x1 = torch.LongTensor(\n","        [env.eos_index] + [env.word2id[w] for w in x1_prefix] + [env.eos_index]\n","    ).view(-1, 1)\n","    len1 = torch.LongTensor([len(x1)])\n","\n","    # Encode the input\n","    with torch.no_grad():\n","        encoded = encoder('fwd', x=x1, lengths=len1, causal=False).transpose(0, 1)\n","\n","    # Beam search decoding\n","    with torch.no_grad():\n","        _, _, beam = decoder.generate_beam(\n","            encoded,\n","            len1,\n","            beam_size=beam_size,\n","            length_penalty=1.0,\n","            early_stopping=1,\n","            max_len=200\n","        )\n","\n","    hypotheses = beam[0].hyp\n","    return f, F, hypotheses\n","\n","from sympy.parsing.sympy_parser import parse_expr\n","\n"],"metadata":{"id":"8_hPnoc9qTgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["F_infix = 'x**(-3)'  # You can replace this with any symbolic function\n","f, F, hypotheses = predict_antiderivatives(F_infix, env, encoder, decoder)\n","\n","# Step 2: Display predictions\n","display_model_predictions(f, F, hypotheses, env)"],"metadata":{"id":"dEgP7exYq6ay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üîß Task: Predict from a Given Derivative Expression (3 points)\n","\n","In this task, you will implement a function that predicts possible antiderivatives F(x)  for a given derivative f(x), using a Transformer-based sequence-to-sequence model.\n","\n","The model is trained to operate on prefix expressions of mathematical functions. Your function must:\n","- Parse an infix string representing  f(x)  into a SymPy expression **without simplifying** it.\n","- Convert the expression to a prefix format suitable for the model.\n"],"metadata":{"id":"g1vZgsuCjQmt"}},{"cell_type":"code","source":["def predict_from_derivative(f_infix, env, encoder, decoder, beam_size=10):\n","    \"\"\"\n","    Given an infix expression for the derivative f(x), predict possible antiderivatives F(x).\n","\n","    Parameters:\n","        f_infix (str): A string in infix notation (e.g., \"x + x**2\")\n","        env: A symbolic environment that provides tokenization and parsing utilities\n","        encoder: The encoder model\n","        decoder: The decoder model\n","        beam_size (int): Number of beams to explore during decoding\n","\n","    Returns:\n","        f (sympy expression): Parsed symbolic expression for the derivative f(x)\n","        None: Placeholder for true F (not known in this setting)\n","        hypotheses (list): List of (score, decoded_tensor) from beam search\n","    \"\"\"\n","    # <YOUR CODE>\n","    f = None\n","    hypotheses = list()\n","    return f, None, hypotheses\n","\n","f, _, hypotheses = predict_from_derivative(\"x+x\", env, encoder, decoder)\n","assert str(f) == \"x + x\", f\"Expected 'x + x', got '{f}'\"\n","\n","display_model_predictions(f, _, hypotheses, env)"],"metadata":{"id":"rPNz3BylUtT-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Explain the resusts for x + x. Why are they not the same as for previous attempts? (1 point.)"],"metadata":{"id":"0BzRentdlr4H"}},{"cell_type":"markdown","source":["## üîç Problem 3: Inspecting the Attention Mechanism (5 Points total)"],"metadata":{"id":"_jXVN8y99n16"}},{"cell_type":"markdown","source":["\n","\n","Recall the formula for self-attention in a Transformer layer:\n","$$\n","\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n","$$\n","\n","Each encoder layer applies this mechanism independently across attention heads.\n"],"metadata":{"id":"XAPbf6DWsFeQ"}},{"cell_type":"markdown","source":["### üîç  Extracting Attention Matrices from the Encoder\n","\n","In order to better understand and visualize what the encoder is focusing on, we define a utility function `get_attention_matrices`. This function uses **forward hooks** in PyTorch to extract the raw attention weights from each attention layer in the encoder during a forward pass.\n","\n","A **hook** is a special mechanism that allows us to tap into the forward (or backward) pass of a model and access intermediate data such as activations or gradients. Here, we register a **forward hook** on each attention module of the encoder, so that when the encoder processes an input, we can capture and store its internal attention matrices.\n","\n","This is particularly useful for:\n","- Visualizing attention patterns over the input sequence\n","- Understanding how different layers attend to various input tokens\n","- Interpreting model behavior and debugging training dynamics\n","\n","The function returns a list of attention matrices, one for each encoder layer.\n","\n","> **Note**: The encoder is run in evaluation mode (`torch.no_grad()`) to avoid affecting the computation graph and to reduce memory usage."],"metadata":{"id":"jGBpKqcnpOZo"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from matplotlib.colors import LinearSegmentedColormap\n","\n","def get_attention_matrices(encoder, input_ids, input_lengths, causal=False):\n","    \"\"\"\n","    Extract attention matrices from encoder for visualization.\n","\n","    Returns:\n","        encoder_attentions: List of attention matrices from each encoder layer\n","    \"\"\"\n","    encoder_attentions = []\n","\n","    # Register hooks to capture attention matrices\n","    def encoder_attention_hook(module, input, output):\n","        # Extract attention weights from the output\n","        attn_weights = module.outputs\n","        encoder_attentions.append(attn_weights.detach())\n","\n","    # Add hooks to all attention modules in encoder\n","    hooks = []\n","    for layer in encoder.attentions:\n","        hook = layer.register_forward_hook(encoder_attention_hook)\n","        hooks.append(hook)\n","\n","    # Forward pass through encoder to capture attention\n","    with torch.no_grad():\n","        encoder_output = encoder('fwd', x=input_ids, lengths=input_lengths, causal=causal)\n","\n","    # Remove hooks\n","    for hook in hooks:\n","        hook.remove()\n","\n","    return encoder_attentions\n"],"metadata":{"id":"bak7i_XQ2x1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_attention(attention_matrix, tokens_in, tokens_out=None, layer_name=\"\", f_expr=None, F_expr=None, figsize=(16, 10)):\n","    \"\"\"\n","    Visualize attention heads in a 4x2 grid with full-sized readable plots.\n","\n","    Args:\n","        attention_matrix: Tensor of shape [heads, tgt_len, src_len]\n","        tokens_in: Tokens on x-axis (input sequence)\n","        tokens_out: Tokens on y-axis (output sequence)\n","        layer_name: Layer title\n","        f_expr: Optional sympy expression for derivative\n","        F_expr: Optional sympy expression for integral\n","        figsize: Size for each full grid (e.g. 16x10 for 8 heads)\n","    \"\"\"\n","    if tokens_out is None:\n","        tokens_out = tokens_in\n","\n","    n_heads = attention_matrix.shape[0]\n","    rows, cols = n_heads // 2, 2  # grid layout: 4x2 for 8 heads\n","\n","    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n","    axes = axes.flatten()\n","\n","    for h in range(n_heads):\n","        ax = axes[h]\n","        data = attention_matrix[h].cpu().numpy()\n","\n","        sns.heatmap(\n","            data,\n","            ax=ax,\n","            cmap=\"Blues\",\n","            annot=True,\n","            fmt=\".2f\",\n","            annot_kws={\"size\": 9},\n","            xticklabels=tokens_in,\n","            yticklabels=tokens_out,\n","            cbar=False,\n","            linewidths=0.5,\n","            linecolor='gray',\n","            vmin=0,\n","            vmax=1\n","        )\n","        ax.set_title(f\"Head {h + 1}\", fontsize=12)\n","        ax.set_xlabel(\"Source\")\n","        ax.set_ylabel(\"Target\")\n","        ax.tick_params(axis='x', labelrotation=45)\n","        ax.tick_params(axis='both', labelsize=10)\n","\n","    # Hide extra axes if any\n","    for h in range(n_heads, len(axes)):\n","        axes[h].axis('off')\n","\n","    # Title text\n","    title = f\"{layer_name}\"\n","    if f_expr is not None and F_expr is not None:\n","        title += f\"\\n$f = {sp.latex(f_expr)}$,\\n$F = {sp.latex(F_expr)}$\"\n","\n","    plt.suptitle(title, y=1.03, fontsize=16)\n","    plt.tight_layout()\n","    plt.subplots_adjust(top=0.88)\n","    return fig\n"],"metadata":{"id":"1H528lwCq8zK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_all_encoder_layers(encoder_attentions, input_tokens, figsize=(15, 20)):\n","    \"\"\"\n","    Visualize attention for each layer in the encoder\n","    \"\"\"\n","    for i, attn in enumerate(encoder_attentions):\n","        # attn shape: [batch_size, num_heads, seq_len, seq_len]\n","        # For visualization, we take the first batch item\n","        attn_matrix = attn[0]  # [num_heads, seq_len, seq_len]\n","\n","        fig = visualize_attention(\n","            attn_matrix,\n","            input_tokens,\n","            layer_name=f\"Encoder Layer {i+1}\",\n","            figsize=figsize\n","        )\n","        plt.show()"],"metadata":{"id":"MeF3IPxTAEwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_model_attention(f_prefix, env, encoder, decoder):\n","    # Prepare input for visualization\n","    x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + f_prefix)\n","    x1 = torch.LongTensor(\n","        [env.eos_index] +\n","        [env.word2id[w] for w in x1_prefix] +\n","        [env.eos_index]\n","    ).view(-1, 1)\n","    len1 = torch.LongTensor([len(x1)])\n","\n","    if not params.cpu:\n","        x1 = x1.cuda()\n","        len1 = len1.cuda()\n","\n","    # Get attention matrices\n","    encoder_attentions = get_attention_matrices(encoder, x1, len1)\n","\n","    # Prepare token labels for visualization\n","    input_tokens = ['<EOS>'] + x1_prefix + ['<EOS>']\n","\n","    # Visualize attention matrices\n","    visualize_all_encoder_layers(encoder_attentions, input_tokens)\n","\n","    return encoder_attentions"],"metadata":{"id":"6csxl21jsUxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Math\n","\n","def run_attention_visualizations(F_infix, env, encoder, decoder):\n","    \"\"\"\n","    Parse a function F, compute its derivative f, tokenize it,\n","    and visualize encoder attention for the model input based on f.\n","\n","    Returns:\n","        encoder_attentions: List of attention matrices\n","        f: SymPy expression (F')\n","        F: SymPy expression (original)\n","        f_prefix: list of tokens (prefix form of f)\n","    \"\"\"\n","    print(\"üîç Analyzing symbolic integration task\")\n","    print(f\"Raw input (F_infix): {F_infix}\")\n","\n","    # Parse function F and compute its derivative f\n","    F = sp.S(F_infix, locals=env.local_dict)\n","    var = list(F.free_symbols)[0]\n","    f = F.diff(var)\n","\n","    print(\"\\nüìå Parsed expressions:\")\n","    display(Math(rf\"F_(x) = {sp.latex(F)}\"))\n","    display(Math(rf\"f(x) = \\frac{{d}}{{dx}}F(x) = {sp.latex(f)}\"))\n","\n","    # Tokenize f to prefix form for the model\n","    f_prefix = env.sympy_to_prefix(f)\n","    print(f\"\\nüßÆ Prefix input to the model (f'): {f_prefix}\")\n","\n","    # Visualize attention matrices\n","    encoder_attentions = visualize_model_attention(f_prefix, env, encoder, decoder)\n","\n","    return encoder_attentions, f, F, f_prefix"],"metadata":{"id":"wmeM9d45s760"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder.eval()\n","decoder.eval()\n","attentions_example=run_attention_visualizations('sin(31*exp(-2*x))', env, encoder, decoder)"],"metadata":{"id":"mdfChxAhs9tV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### üîç Task: Interpret Transformer Attention Heads (5 Points)\n","\n","In this assignment, you will explore and interpret what some attention heads are doing in a pretrained transformer model that attempts to generate antiderivatives of symbolic functions.\n","\n","---\n","\n","### üéØ Objective:\n","\n","Choose **4 attention heads** from any layers and try to hypothesize what each of them might be focusing on.\n","\n","For each head, you must:\n","- Specify the **layer number** and **head number**\n","- Provide a **plausible explanation** of what the head might be attending to do\n","- Use your judgment to back the claim ‚Äî even speculative insights are welcome!\n","\n","‚ö†Ô∏è Keep in mind:  \n","Most heads are not cleanly interpretable ‚Äî and that‚Äôs okay. The goal is to practice the **process** of inspection and reasoning.\n","\n","---\n","\n","### üß™ Default setup:\n","\n","By default, you can use the input function:  \n","$$\n","\\sin(31 \\cdot e^{-2x})\n","$$\n","\n","This function is complex enough to exhibit interesting symbolic structure.\n","Around first 3 layers you can definetly find insightfull layers.\n","---\n","\n","### üõ†Ô∏è Notes:\n","\n","- You are welcome (and encouraged!) to try **your own function inputs** instead of the default.\n","- If you do, make sure to include **all required outputs and visuals** to support your interpretation (e.g., tokenized inputs, attention weights).\n","- Try to reflect on whether the heads are focusing on:\n","  - Parenthesis structure?\n","  - Operator precedence?\n","  - Specific mathematical symbols like `x`, `e`, `sin`?\n","  - Repetitive patterns?\n","  - Numbers\n","  - Signs\n","\n","---\n","\n","### ‚úÖ Submission format (for each head):\n","\n","Please follow this format for each of your 4 analyses:\n","\n","```\n","Layer: <layer_number>\n","Head: <head_number>\n","\n","üß† Interpretation:\n","<your explanation>\n","\n","üîç Evidence:\n","<describe what makes you think that ‚Äì show attention maps or token examples if possible>\n","```\n","\n","---\n","\n","Happy decoding! üß†üî¨"],"metadata":{"id":"z3sojShqnKOt"}},{"cell_type":"markdown","source":["## üîÅ Bonus Task. Cross-Attention in Transformer Decoders (5 points total)\n","\n","In contrast to **self-attention**, which attends to tokens within the same sequence, **cross-attention** occurs in the decoder and allows it to attend to the encoder's output representations.\n","\n","This mechanism is crucial for tasks like translation, summarization, or symbolic integration, where the decoder needs to align its generated output with specific parts of the input.\n","\n","### üîç What We'll Do\n","\n","In this section, we will:\n","- Capture the **cross-attention matrices** from all decoder layers.\n","- Use forward hooks on the decoder's cross-attention modules (`encoder_attn`) to collect attention weights during autoregressive generation.\n","- For each decoder layer, we will obtain an attention tensor of shape:\n","$$\n","\\texttt{[batch, heads, target_len, source_len]}\n","$$\n","These matrices allow us to analyze how the decoder focuses on different parts of the input sequence while generating each token.\n","\n","We wrap this functionality in the `get_cross_attention_matrices` function."],"metadata":{"id":"Jz1CdzD29XUR"}},{"cell_type":"markdown","source":["\n","### üß™ Implementation Task: Extracting Cross-Attention Weights (4 points)\n","\n","To analyze how the decoder attends to the encoder's outputs, complete the function `get_cross_attention_matrices`.\n","\n","This function will:\n","- Register **forward hooks** on each decoder cross-attention (`encoder_attn`) block to capture attention weights during generation.\n","- Run the encoder and then the decoder to generate a sequence.\n","- Collect the cross-attention tensors across decoder layers.\n","\n","The function should return:\n","- `cross_attentions`: A list with one tensor per decoder layer of shape `[batch, heads, target_len, source_len]`.\n","- `generated`: The generated sequence (token IDs).\n","- `gen_len`: The length(s) of generated sequences.\n","\n","The missing parts are clearly marked as `<YOUR CODE>` ‚Äî fill them in using your knowledge of encoder-decoder workflows in PyTorch.\n","\n","\n","This task is worth **5 points**."],"metadata":{"id":"OcOKbjbLrhtV"}},{"cell_type":"code","source":["def get_cross_attention_matrices(encoder, decoder, input_ids, input_lengths, causal=False):\n","    \"\"\"\n","    Extract cross-attention weights from decoder during generation.\n","\n","    Args:\n","        encoder: Transformer encoder\n","        decoder: Transformer decoder\n","        input_ids: LongTensor of shape [seq_len, batch]\n","        input_lengths: LongTensor of shape [batch]\n","        causal: Whether to use causal attention (not needed here)\n","\n","    Returns:\n","        cross_attentions: List of tensors, one per decoder layer,\n","                          each of shape [batch, heads, tgt_len, src_len]\n","        generated: Tensor of generated token ids\n","        gen_len: Lengths of generated sequences\n","    \"\"\"\n","    cross_attn_chunks = []\n","\n","    def cross_attention_hook(module, input, output):\n","        attn_weights = module.outputs  # shape: [batch, heads, 1, src_len] per timestep\n","        cross_attn_chunks.append(attn_weights.detach())\n","\n","    # Register hooks on all cross-attention layers in decoder\n","    hooks = [layer.register_forward_hook(cross_attention_hook) for layer in decoder.encoder_attn]\n","\n","    # Run encoder and decoder\n","    with torch.no_grad():\n","        # <YOUR CODE>\n","\n","    # Remove hooks\n","    for hook in hooks:\n","        hook.remove()\n","\n","    # <YOUR CODE>\n","\n","    return cross_attentions, generated, gen_len"],"metadata":{"id":"oj8K1pdht97C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sympy import simplify\n","from IPython.display import display, Math\n","\n","def visualize_model_cross_attention(f_prefix, env, encoder, decoder):\n","    # Prepare input for visualization\n","    x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + f_prefix)\n","    x1 = torch.LongTensor(\n","        [env.eos_index] +\n","        [env.word2id[w] for w in x1_prefix] +\n","        [env.eos_index]\n","    ).view(-1, 1)\n","    len1 = torch.LongTensor([len(x1)])\n","\n","    if not params.cpu:\n","        x1 = x1.cuda()\n","        len1 = len1.cuda()\n","\n","    # Get attention matrices and generated output\n","    encoder_cross_attentions, generated, gen_len = get_cross_attention_matrices(encoder, decoder, x1, len1)\n","\n","    # Prepare input token labels\n","    input_tokens = ['<EOS>'] + x1_prefix + ['<EOS>']\n","\n","    # Prepare output tokens for visualization (1 per decoder step)\n","    output_tokens = []\n","    for i in range(generated.shape[0]):\n","        output_tokens.append([env.id2word[idx.item()] for idx in generated[i]][0])\n","\n","    print(\"\\nüîÆ Predicted Function (from first beam):\")\n","    pred_ids = generated[:, 0].tolist()\n","    pred_tokens = [env.id2word[wid] for wid in pred_ids if env.id2word[wid] not in ['<s>', '<EOS>', '<pad>']]\n","\n","    try:\n","        pred_infix = env.prefix_to_infix(pred_tokens)\n","        pred_sympy = env.infix_to_sympy(pred_infix)\n","\n","        # Try to validate against f\n","        f = sp.S(env.prefix_to_infix(f_prefix), locals=env.local_dict)\n","        var = list(f.free_symbols)[0]\n","        res = \"OK\" if simplify(pred_sympy.diff(var) - f, seconds=1) == 0 else \"NO\"\n","\n","        label = f\"{res}\"\n","        display(Math(rf\"\\text{{{label}}} \\quad \\Rightarrow \\quad {sp.latex(pred_sympy)}\"))\n","\n","    except Exception:\n","        display(Math(rf\"\\text{{INVALID PREFIX EXPRESSION}} \\quad \\Rightarrow \\quad \\text{{{pred_tokens}}}\"))\n","\n","    # === Visualize cross attention ===\n","    visualize_all_cross_layers(encoder_cross_attentions, input_tokens, output_tokens)\n","\n","    return encoder_cross_attentions"],"metadata":{"id":"i9unjTO-9usz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_all_cross_layers(cross_attentions, input_tokens, output_tokens, figsize=(15, 20)):\n","    \"\"\"\n","    Visualize attention for each layer in the encoder and decoder\n","    \"\"\"\n","\n","    for i, attn in enumerate(cross_attentions):\n","        # attn shape: [batch_size, num_heads, seq_len, seq_len]\n","        # For visualization, we take the first batch item\n","        attn_matrix = attn[0]  # [num_heads, seq_len, seq_len]\n","\n","        fig = visualize_attention(\n","            attn_matrix,\n","            input_tokens,\n","            output_tokens,\n","            layer_name=f\"Encoder Layer {i+1}\",\n","            figsize=figsize\n","        )\n","        plt.show()\n","\n","from IPython.display import display, Math\n","\n","def run_cross_attention_visualizations(F_infix, env, encoder, decoder):\n","    \"\"\"\n","    Visualize cross-attention maps for decoder attending to encoder representations,\n","    given a symbolic integration task.\n","    \"\"\"\n","    print(\"üîç Running cross-attention visualization\")\n","    print(f\"Raw input (F_infix): {F_infix}\")\n","\n","    # Parse and differentiate\n","    F = sp.S(F_infix, locals=env.local_dict)\n","    var = list(F.free_symbols)[0]\n","    f = F.diff(var)\n","\n","    # Show parsed math\n","    print(\"\\nüìå Parsed symbolic expressions:\")\n","    display(Math(rf\"F(x) = {sp.latex(F)}\"))\n","    display(Math(rf\"f(x) = \\frac{{d}}{{dx}}F(x) = {sp.latex(f)}\"))\n","\n","    # Tokenize for input\n","    f_prefix = env.sympy_to_prefix(f)\n","    print(f\"\\nüßÆ Tokenized prefix input to model (f'): {f_prefix}\")\n","\n","    # Visualize cross-attention\n","    cross_attentions = visualize_model_cross_attention(f_prefix, env, encoder, decoder)\n","\n","    return cross_attentions"],"metadata":{"id":"T0pPRgTw95ES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ‚ûï Bonus Task: Cross-Attention Exploration (1 pts)\n","\n","Investigate the **cross-attention** mechanism between the encoder and decoder.\n","\n","Your task:\n","\n","1. **Pick one cross-attention head** and describe what kind of encoder tokens the decoder attends to at that position.\n","2. **Give a concrete example** (e.g., decoder token \"sin\" attends to what? Show attention weights or describe the pattern).\n","\n","üí° Tip: Try it on the default input ($\\sin(31 \\cdot e^{-2x})$) or any other expression you analyzed earlier.\n","\n","üéØ Focus on intuition ‚Äî even rough insights are useful!"],"metadata":{"id":"y5D1DWDAqP_K"}},{"cell_type":"code","source":["attentions_example=run_cross_attention_visualizations('sin(31*exp(-2*x))', env, encoder, decoder)"],"metadata":{"id":"yovJXMSj963A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e-fAooFbq_Am"},"execution_count":null,"outputs":[]}]}