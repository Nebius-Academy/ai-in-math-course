{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G3o6AlW0wwlZCpqJYGKgCuiKHh-KgS-T","timestamp":1746150245481}],"authorship_tag":"ABX9TyPRyChYDys+e/ivGON9elh7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3gD7AXfuU0js"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import matplotlib.pyplot as plt\n","import random\n","import torch.optim as optim\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","random.seed(42)\n","np.random.seed(42)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","source":["# Problem 1(5 points)"],"metadata":{"id":"ADwMBSgxVMNV"}},{"cell_type":"markdown","source":["## Problem Description\n","\n","We will apply **Natural Language Processing (NLP)** methods to perform a **binary classification task**: determining whether a given number `x` is divisible by 3.\n","\n","The input `x` is provided as a sequence of **digits**, with each digit treated as a separate **input token**. Using this tokenized representation, the model learns to classify the input sequence as:\n","\n","- `1` (True) if `x` is divisible by 3  \n","- `0` (False) otherwise\n"],"metadata":{"id":"FB6_rFOdnN5B"}},{"cell_type":"code","source":["INPUT_SIZE = 10  # One-hot encoding for digits 0-9\n","HIDDEN_SIZE = 64\n","OUTPUT_SIZE = 2  # Binary classification: divisible by 3 or not\n","BATCH_SIZE = 128\n","EPOCHS = 300\n","LEARNING_RATE = 0.001\n","SEQ_LENGTH = 8  # Maximum length of input number"],"metadata":{"id":"CQFdp_05U6he"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","The following function, `generate_data`, creates synthetic labeled data for training a model to classify whether a number is divisible by 3. It operates as follows:\n","\n","### Function: `generate_data(num_samples, max_length=SEQ_LENGTH)`\n","\n","- **Inputs:**\n","  - `num_samples`: Number of data samples to generate.\n","  - `max_length`: Maximum number of digits per sample (default is `SEQ_LENGTH`).\n","\n","- **Process:**\n","  1. For each sample, a random sequence of digits is generated with a length between 1 and `max_length`.\n","  2. The sequence is **zero-padded** at the beginning so that all samples have the same length (`max_length`), which is necessary for batch processing in neural networks.\n","  3. The label is determined based on whether the **sum of the digits** is divisible by 3 â€” a standard rule for checking divisibility by 3.\n","\n","- **Output:**\n","  - `X`: A NumPy array of shape `(num_samples, max_length)` containing the padded digit sequences.\n","  - `y`: A NumPy array of shape `(num_samples,)` containing binary labels:  \n","    - `1` if the number is divisible by 3  \n","    - `0` otherwise\n"],"metadata":{"id":"rIGMslT8o8G9"}},{"cell_type":"code","source":["def generate_data(num_samples, max_length=SEQ_LENGTH):\n","    X = []\n","    y = []\n","\n","    for _ in range(num_samples):\n","        # <YOUR CODE>\n","\n","    return np.array(X), np.array(y)"],"metadata":{"id":"eEaI6Oc-pIzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def one_hot_encode(X):\n","    # Create one-hot encoding for digits 0-9\n","    X_one_hot = np.zeros((X.shape[0], X.shape[1], INPUT_SIZE))\n","    for i in range(X.shape[0]):\n","        for j in range(X.shape[1]):\n","            if X[i, j] > 0:  # Skip padding zeros\n","              X_one_hot[i, j, X[i, j]] = 1\n","    return X_one_hot\n","\n","\n","# <YOUR CODE>: REWRITE THE FOLLOWING PART USING split without repetitions of functions.\n","\n","# train_X, train_y = generate_data(10000)\n","# val_X, val_y = generate_data(2000)\n","# test_X, test_y = generate_data(1000)\n","\n","# train_X_one_hot = one_hot_encode(train_X)\n","# val_X_one_hot = one_hot_encode(val_X)\n","# test_X_one_hot = one_hot_encode(test_X)\n","\n","# # Convert to PyTorch tensors\n","# train_X_tensor = torch.FloatTensor(train_X_one_hot)\n","# train_y_tensor = torch.LongTensor(train_y)\n","# val_X_tensor = torch.FloatTensor(val_X_one_hot)\n","# val_y_tensor = torch.LongTensor(val_y)\n","# test_X_tensor = torch.FloatTensor(test_X_one_hot)\n","# test_y_tensor = torch.LongTensor(test_y)"],"metadata":{"id":"vH26udb3VTY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n","val_dataset = TensorDataset(val_X_tensor, val_y_tensor)\n","test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n","\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"],"metadata":{"id":"bF_VgY6NVhPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DivisibilityRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(DivisibilityRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n","        # <YOUR CODE>\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        # Initialize hidden state\n","        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n","\n","        # Forward propagate the RNN\n","        out, _ = self.rnn(x, h0)\n","\n","        # Only use the output from the last time step\n","        out = # <YOUR CODE>\n","        return out"],"metadata":{"id":"M0k79bpvWBk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = # <YOUR CODE>\n","            loss = # <YOUR CODE>\n","            # <YOUR CODE>\n","\n","            train_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = train_loss / len(train_loader)\n","        train_accuracy = 100 * correct / total\n","        train_losses.append(train_loss)\n","        train_accuracies.append(train_accuracy)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                outputs = # <YOUR CODE>\n","                loss = # <YOUR CODE>\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(val_loader)\n","        val_accuracy = 100 * correct / total\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","        if epoch % 10 == 9:\n","          print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies"],"metadata":{"id":"PrrccHldWE1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Create model, loss function and optimizer\n","model = DivisibilityRNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Train the model\n","train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n","    model, train_loader, val_loader, criterion, optimizer, EPOCHS\n",")\n","\n","# Plot training and validation metrics\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Training and Validation Loss')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_accuracies, label='Training Accuracy')\n","plt.plot(val_accuracies, label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.legend()\n","plt.title('Training and Validation Accuracy')\n","plt.tight_layout()\n","plt.show()\n","\n","# Evaluate on test set\n","model.eval()\n","test_loss = 0\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_loss = test_loss / len(test_loader)\n","test_accuracy = 100 * correct / total\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n","\n","# Test with specific examples\n","def test_number(model, number):\n","    # Convert number to digits\n","    digits = [int(d) for d in str(number)]\n","\n","    # Pad with zeros\n","    padded_digits = np.zeros(SEQ_LENGTH, dtype=int)\n","    padded_digits[-len(digits):] = digits\n","\n","    # Convert to one-hot encoding\n","    one_hot = np.zeros((1, SEQ_LENGTH, INPUT_SIZE))\n","    for j in range(SEQ_LENGTH):\n","        if padded_digits[j] > 0:\n","            one_hot[0, j, padded_digits[j]] = 1\n","\n","    # Convert to tensor\n","    input_tensor = torch.FloatTensor(one_hot)\n","\n","    # Predict\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(input_tensor)\n","        _, predicted = torch.max(output.data, 1)\n","\n","    is_divisible_by_3 = number % 3 == 0\n","    prediction = bool(predicted.item())\n","\n","    print(f\"Number: {number}\")\n","    print(f\"Actually divisible by 3: {is_divisible_by_3}\")\n","    print(f\"Model prediction: {prediction}\")\n","    print(f\"Prediction {'correct' if is_divisible_by_3 == prediction else 'incorrect'}\\n\")\n","\n","# Test some examples\n","test_number(model, 123)  # Divisible by 3 (1+2+3=6, 6%3=0)\n","test_number(model, 456)  # Divisible by 3 (4+5+6=15, 15%3=0)\n","test_number(model, 7890) # Divisible by 3 (7+8+9+0=24, 24%3=0)\n","test_number(model, 12)   # Divisible by 3 (1+2=3, 3%3=0)\n","test_number(model, 5)    # Not divisible by 3 (5%3=2)\n","test_number(model, 1234) # Not divisible by 3 (1+2+3+4=10, 10%3=1)"],"metadata":{"id":"BNw01RBsWIky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# YOUR Baseline is 95% Validation accuracy\n","\n","\n"],"metadata":{"id":"2ZUByt_3pQD9"}},{"cell_type":"markdown","source":["# Problem 2 (5 Points)"],"metadata":{"id":"eRJXYgZmYRoO"}},{"cell_type":"markdown","source":["## Problem Description\n","\n","We will apply **Natural Language Processing (NLP)** methods to learn a sequence-to-sequence mapping task: predicting the output sequence corresponding to an input sequence of digits.\n","\n","Specifically, the model will be trained to predict `3x` given `x`, where `x` is a sequence of digits. Each digit is treated as an individual **input token**, and the model learns to generate the sequence of digits representing `3x` as the output.\n","\n","This setup treats digit sequences as a language and leverages NLP architectures such as **LSTM** to model the transformation."],"metadata":{"id":"s29DEnhOlWu-"}},{"cell_type":"markdown","source":["**LSTM** is a special type of Recurrent Neural Network (RNN) designed to capture long-term dependencies and mitigate the vanishing gradient problem found in traditional RNNs.\n","\n","> ðŸ§  **Note:** In our problem, we will use an **LSTM** instead of a vanilla **RNN** to better handle long-range dependencies in the sequence data.\n","\n","## Key Features\n","\n","- Designed to remember information for long periods.\n","- Uses gates to control the flow of information.\n","- Effective for tasks like language modeling, time-series forecasting, and machine translation.\n","\n","## LSTM Cell Structure\n","\n","An LSTM cell contains the following components:\n","\n","1. **Forget Gate** (`f_t`): Decides what information to discard from the cell state.\n","   $$\n","   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n","   $$\n","\n","2. **Input Gate** (`i_t`) and **Candidate Cell State** (`\\tilde{C}_t`): Decide what new information to store.\n","   $$\n","   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n","   $$\n","   $$\n","   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n","   $$\n","\n","3. **Cell State Update** (`C_t`):\n","   $$\n","   C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n","   $$\n","\n","4. **Output Gate** (`o_t`) and Hidden State (`h_t`): Decide the output.\n","   $$\n","   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n","   $$\n","   $$\n","   h_t = o_t \\cdot \\tanh(C_t)\n","   $$\n","\n","## LSTM Cell Diagram\n","\n","![LSTM Cell](https://drive.google.com/uc?export=view&id=1ddc2g4NFCy4Tt-K0Qu6-2htu5B2VHJEM)\n","\n","## Advantages over Vanilla RNNs\n","\n","- Better at preserving long-term dependencies.\n","- Reduces vanishing gradient issues.\n","- Widely used in NLP and sequence modeling."],"metadata":{"id":"oa8xFmbgjPuY"}},{"cell_type":"code","source":["input_size = 10  # 0-9 digits\n","hidden_size = 64\n","num_layers = 1\n","output_size = 10  # 0-9 digits\n","batch_size = 128\n","learning_rate = 0.1\n","num_epochs = 10\n"],"metadata":{"id":"3Ex9xRUJhKR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_dataset(size=10000):\n","    X = []\n","    y = []\n","\n","    # Generate numbers from 1 to 9999\n","    numbers = random.sample(range(1, 10000), size)\n","\n","    for num in numbers:\n","        # Format input as 4 digits with leading zeros if needed\n","        input_digits = f\"{num:04d}\"\n","\n","        # Calculate the result (multiply by 3)\n","        result = num * 3\n","\n","        # Format output as 5 digits with leading zeros if needed\n","        output_digits = f\"{result:05d}\"\n","\n","        # Convert to digit lists\n","        input_seq = [int(d) for d in input_digits]\n","        output_seq = [int(d) for d in output_digits]\n","\n","        # reverse numbers\n","        input_seq.reverse()\n","        output_seq.reverse()\n","\n","        X.append(input_seq)\n","        y.append(output_seq)\n","\n","    return X, y\n","\n","# Custom Dataset class\n","class MultiplicationDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        # One-hot encode the input and output\n","        input_tensor = torch.zeros(len(self.X[idx]), input_size)\n","        target_tensor = torch.zeros(len(self.y[idx]), output_size)\n","\n","        for i, digit in enumerate(self.X[idx]):\n","            input_tensor[i, digit] = 1\n","\n","        for i, digit in enumerate(self.y[idx]):\n","            target_tensor[i, digit] = 1\n","\n","        return input_tensor, target_tensor\n","\n","# Encoder model\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True,\n","                          batch_first=True)\n","\n","\n","    def forward(self, x):\n","        # x shape: [batch_size, seq_len, input_size]\n","        output, hidden = self.rnn(x)\n","        return output, hidden\n","\n","# Decoder model\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size, rnn_type='LSTM'):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True,\n","                          batch_first=True)\n","\n","        self.out = nn.Linear(hidden_size*2, output_size)\n","\n","    def forward(self, x, hidden):\n","        # <YOUR CODE>\n","\n","\n","\n","\n"],"metadata":{"id":"vQAVVE0NYx0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Teacher Forcing\n","\n","**Teacher forcing** is a technique used to train RNNs where the true output from the training data is fed as the next input, instead of the model's own prediction.\n","\n","- It helps the model learn faster and more accurately.\n","- During training: use the correct previous output.\n","- During testing: the model must use its own predictions.\n","\n","**Pros**: Faster learning, better early training.  \n","**Cons**: May cause errors during inference due to reliance on correct inputs."],"metadata":{"id":"JmlJIpYBNjKz"}},{"cell_type":"code","source":["# Seq2Seq model\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, teacher_forcing_ratio=0.5):\n","        # src shape: [batch_size, src_seq_len, input_size]\n","        batch_size = src.size(0)\n","        target_len = 5  # Output is always 5 digits\n","        target_size = self.decoder.out.out_features\n","\n","        # Tensor to store decoder outputs\n","        outputs = torch.zeros(batch_size, target_len, target_size).to(self.device)\n","\n","        # Encode the source sequence\n","        _, hidden = self.encoder(src)\n","\n","        # First input to the decoder is the <SOS> token (represented as all zeros)\n","        decoder_input = torch.zeros(batch_size, 1, target_size).to(self.device)\n","\n","        # Decode one step at a time\n","        for t in range(target_len):\n","            output, hidden = self.decoder(decoder_input, hidden)\n","            outputs[:, t:t+1, :] = output\n","\n","            # Teacher forcing\n","            use_teacher_forcing = random.random() < teacher_forcing_ratio\n","\n","            # Get the highest predicted token\n","            top1 = output.argmax(2)\n","\n","            # If teacher forcing, use actual next token as next input\n","            # If not, use predicted token\n","            if use_teacher_forcing and t < target_len - 1:\n","                # Use target as next input\n","                # <YOUR CODE>\n","            else:\n","                # Use prediction as next input\n","                # <YOUR CODE>\n","\n","        return outputs\n","\n","# Training function\n","def train(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio=0.5):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for batch_idx, (src, target) in enumerate(train_loader):\n","        src, target = src.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(src, teacher_forcing_ratio)\n","\n","        # Reshape output and target for loss calculation\n","        output_dim = output.shape[-1]\n","        output = output.view(-1, output_dim)\n","        target = target.view(-1, output_dim)\n","\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(train_loader)"],"metadata":{"id":"YnAPV1w0NeAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, val_loader, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (src, target) in enumerate(val_loader):\n","            src, target = src.to(device), target.to(device)\n","\n","            output = model(src, 0)  # No teacher forcing during evaluation\n","\n","            # Reshape output and target for loss calculation\n","            output_dim = output.shape[-1]\n","            output = output.view(-1, output_dim)\n","            target = target.view(-1, output_dim)\n","\n","            loss = criterion(output, target)\n","            epoch_loss += loss.item()\n","\n","            # Calculate accuracy\n","            pred = output.argmax(dim=1)\n","            target_indices = target.argmax(dim=1)\n","            correct_predictions += (pred == target_indices).sum().item()\n","            total_predictions += pred.size(0)\n","\n","    accuracy = correct_predictions / total_predictions\n","    return epoch_loss / len(val_loader), accuracy\n","\n","def test_model(model, num_examples=5):\n","    model.eval()\n","    results = []\n","\n","    with torch.no_grad():\n","        for _ in range(num_examples):\n","            # Generate a random number\n","            num = random.randint(1, 9999)\n","            input_digits = f\"{num:04d}\"[::-1]\n","\n","            # Expected output\n","            expected = num * 3\n","            expected_digits = f\"{expected:05d}\"[::-1]\n","\n","            # Prepare input tensor\n","            input_tensor = torch.zeros(1, 4, input_size)\n","            for i, digit in enumerate(input_digits):\n","                input_tensor[0, i, int(digit)] = 1\n","\n","            input_tensor = input_tensor.to(device)\n","\n","            # Forward pass\n","            output = model(input_tensor, 0)\n","\n","            # Get predicted digits\n","            predicted_indices = output.argmax(dim=2).squeeze().cpu().numpy()\n","            predicted_digits = ''.join(str(idx) for idx in predicted_indices)\n","\n","            results.append({\n","                'input': input_digits[::-1],\n","                'expected': expected_digits[::-1],\n","                'predicted': predicted_digits[::-1],\n","                'correct': expected_digits == predicted_digits\n","                        })\n","\n","    return results"],"metadata":{"id":"AsxMd2mSY5Qy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main(params):\n","    input_size = params['input_size']\n","    hidden_size = params['hidden_size']\n","    num_layers = params['num_layers']\n","    output_size = params['output_size']\n","    batch_size = params['batch_size']\n","    learning_rate = params['learning_rate']\n","    num_epochs = params['num_epochs']\n","\n","\n","    # Generate dataset\n","    X_data, y_data = generate_dataset(size=8000)\n","\n","    # Split into train and validation sets\n","    split = int(0.8 * len(X_data))\n","    X_train, X_val = # <YOUR CODE>\n","    y_train, y_val = # <YOUR CODE>\n","\n","    # Create datasets\n","    train_dataset = MultiplicationDataset(X_train, y_train)\n","    val_dataset = MultiplicationDataset(X_val, y_val)\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Initialize model\n","    model = # <YOUR CODE>\n","\n","    # Initialize optimizer and criterion\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    # Training loop\n","    train_losses = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    for epoch in range(num_epochs):\n","        train_loss = train(model, train_loader, optimizer, criterion, device)\n","        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Epoch: {epoch+1}, \\tTrain Loss: {train_loss:.4f}, \\tVal Loss: {val_loss:.4f}, \\tVal Accuracy: {val_accuracy:.4f}')\n","\n","    # Plot training and validation loss\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Training and Validation Loss')\n","    plt.savefig('loss_plot.png')\n","\n","    # Plot validation accuracy\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.title('Validation Accuracy')\n","    plt.savefig('accuracy_plot.png')\n","\n","    # Test the model on some examples\n","    test_results = test_model(model, 20)\n","\n","    print(\"\\nTest Results:\")\n","    for result in test_results:\n","        status = \"âœ“\" if result['correct'] else \"âœ—\"\n","        print(f\"{result['input']} * 3 = {result['expected']} | Predicted: {result['predicted']} {status}\")\n","\n","    # Calculate overall accuracy\n","    correct = sum(1 for result in test_results if result['correct'])\n","    print(f\"\\nTest Accuracy: {correct / len(test_results):.2f}\")\n","\n","    # Save the model\n","    torch.save(model.state_dict(), 'multiplication_model.pth')\n","    print(\"Model saved as 'multiplication_model.pth'\")"],"metadata":{"id":"1GzDbELXY82P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = {\n","  'input_size': 10,  # 0-9 digits\n","  'hidden_size': 64,\n","  'num_layers': 1,\n","  'output_size': 10,  # 0-9 digits\n","  'batch_size': 128,\n","  'learning_rate': 0.1,\n","  'num_epochs': 20\n","}\n","\n","main(params)"],"metadata":{"id":"eNRs2zLDZIqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# YOUR Baseline is 95% Validation accuracy"],"metadata":{"id":"cQMMDvVoqb3b"}}]}