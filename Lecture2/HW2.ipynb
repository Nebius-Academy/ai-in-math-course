{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Homework 2\n","\n","Disclaimer: Feel free to include any necessary imports below as needed."],"metadata":{"id":"8U6RuehtXDBF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-spj8YeC3Oyz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# Fix the seed and the random state\n","seed=42\n","random_state=42\n","np.random.seed(seed)"]},{"cell_type":"markdown","source":["# Qubics (5 Points)"],"metadata":{"id":"D-DdiLPn8cBd"}},{"cell_type":"markdown","source":["\n","\n","In this task, our goal is to construct a **classifier** that implements **logistic regression** to **predict the nature of the roots** of a **cubic equation** of the form:\n","\n","$\n","x^3 + p x + q = 0$\n","\n","Note that the $ x^2 $ term is omitted **for simplicity**, which still allows the equation to exhibit a rich variety of root structures while reducing the number of parameters.\n","\n","Specifically, we aim to classify each equation into one of two categories based on the type of its real roots:\n","\n","- **Three distinct real roots**\n","- **One real root and a pair of complex conjugate roots**\n","\n","This classification depends on the **discriminant** of the cubic equation. Our objective is to train a model that learns the relationship between the coefficients $ p $ and $ q $, and the corresponding root structure, effectively learning to **rediscover the discriminant-based decision boundary** through data."],"metadata":{"id":"H4JbDKEGDEaA"}},{"cell_type":"code","source":["# Generate dataset\n","n_samples = 10000\n","MAX_VAL = 10\n","p = np.random.uniform(-MAX_VAL, MAX_VAL, n_samples)\n","q = np.random.uniform(-MAX_VAL, MAX_VAL, n_samples)\n","# x^3 + p x + q = 0\n","\n","\n","# Calculate discriminant\n","delta_dep = [0] * n_samples\n","for i, (pi, qi) in enumerate(zip(p, q)):\n","   delta_dep[i] =  -4 * pi**3 -27 * qi**2\n","delta_dep = np.array(delta_dep)\n","\n","# Label data: 1 for 3 real roots, 0 for 1 real root\n","labels_dep = np.where(delta_dep < 0, 1, 0)\n","\n","# Create dataframe\n","df_dep = pd.DataFrame({'p': p, 'q': q, 'label': labels_dep})\n","\n","# Prepare data\n","X_dep = df_dep[['p', 'q']]\n","y_dep = df_dep['label']"],"metadata":{"id":"EjjxXZd65j3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count the occurrences of each label\n","label_counts = y_dep.value_counts()\n","\n","# Create the pie chart\n","plt.figure(figsize=(6, 6))\n","plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n","plt.title('Distribution of Labels (y)')\n","plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n","plt.show()"],"metadata":{"id":"--bX6Y705qXD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into train and test sets\n","X_train, X_test, y_train, y_test = #<YOUR CODE HERE>\n","\n","# Initialize lists to store accuracies and polynomial degrees\n","degrees = range(1, 6)\n","accuracies = []\n","\n","# Train and evaluate polynomial logistic regression models for different degrees\n","for degree in degrees:\n","    #<YOUR CODE HERE>\n","    # Try to use make_pipeline\n","\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracies.append(accuracy)\n","    print(f'Degree: {degree}, Accuracy: {accuracy}')\n","\n","# Plot the results\n","plt.plot(degrees, accuracies, marker='o')\n","plt.xlabel('Polynomial Degree')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy of Polynomial Logistic Regression vs. Degree')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"jOAbOfuS6WHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_degree = #<YOUR CODE HERE>\n","\n","best_model = #<YOUR CODE HERE>\n","\n","# Predict on the test set\n","y_pred = best_model.predict(X_test)"],"metadata":{"id":"78YWY4Bq7oWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","\n","# Scatter plot of test data points\n","plt.scatter(X_test['p'], X_test['q'], c=y_pred, cmap='viridis', label='Predicted Labels', alpha=0.5)\n","\n","# Generate points for the curve\n","p_curve = np.linspace(-MAX_VAL, MAX_VAL, 400)\n","q_curve = np.sqrt(-4 * p_curve**3 / 27)\n","plt.plot(p_curve, q_curve, color='red', label='-4 * pi**3 - 27 * qi**2 = 0')\n","plt.plot(p_curve, -q_curve, color='red')\n","\n","\n","plt.xlabel('p')\n","plt.ylabel('q')\n","plt.title('2D Scatter Plot of Predictions with Curve')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"aPk9oc3W7qKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# $2 \\times 2$ Matrices Classification (5 Points)"],"metadata":{"id":"QSlY6YJy9F0u"}},{"cell_type":"markdown","source":["<!-- ## Task Overview: Identifying Group Structures Among Latin Squares -->\n","\n","In this task, we aim to develop a **classifier** that can distinguish between **$2 \\times 2$ integer matrices** that are **singular** (i.e., have determinant zero) and those that are **non-singular** (i.e., have non-zero determinant).\n","\n","Each matrix has entries drawn from the integer range $-10$ to $10$, giving us $21^4$ possible matrices in total. For a $2 \\times 2$ matrix:\n","\n","$\n","\\begin{bmatrix}\n","a & b \\\\\n","c & d\n","\\end{bmatrix},\n","$\n","\n","the determinant is calculated as:\n","\n","$\n","\\det = ad - bc\n","$\n","\n","Notably, the majority of randomly generated matrices in this space will have a **non-zero determinant**. The number of matrices for which \\(ad = bc\\) (i.e., the determinant is zero) is relatively small, creating a **class imbalance** problem.\n","\n","The goal is to train a binary **logistic regression**‚Äîto predict whether a given matrix is singular or not. However, because the dataset is imbalanced, a naive model might achieve high **accuracy** simply by always predicting the majority class (non-singular). This would be misleading, as it would fail to detect the minority class (singular matrices), which is often the class of interest in applications such as system solvability, matrix inversion, or numerical stability.\n","\n"],"metadata":{"id":"9PdPeq_79Mwz"}},{"cell_type":"code","source":["import numpy as np\n","from itertools import product\n","\n","def generate_integer_dataset(n_samples=1000, value_range=(-10, 10), seed=42):\n","    rng = np.random.default_rng(seed)\n","\n","    values = list(range(value_range[0], value_range[1] + 1))\n","    all_matrices = list(product(values, repeat=4))  # (a, b, c, d)\n","\n","    labels = []\n","    class_1 = []\n","\n","    for row in all_matrices:\n","        a, b, c, d = row\n","        det = a * d - b * c\n","        if det == 0:\n","            labels.append(0)\n","        else:\n","            labels.append(1)\n","\n","    return np.array(all_matrices), np.array(labels)"],"metadata":{"id":"I4hDMqIaJYLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_int, y_int = generate_integer_dataset()\n","\n","labels, counts = np.unique(y_int, return_counts=True)\n","\n","# Plot pie chart\n","plt.figure(figsize=(6, 6))\n","plt.pie(counts, labels=[f'Class {label}' for label in labels], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])\n","plt.title('Distribution of Labels in y_int')\n","plt.axis('equal')  # Equal aspect ratio makes the pie a circle.\n","plt.show()"],"metadata":{"id":"txhzieSPudZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We generated a collection of such matrices, and the resulting classes are **highly imbalanced**. This means we need to be **especially cautious** when evaluating model performance.\n","\n","For example, a naive classifier that always predicts class `1` might achieve a **low overall error**, simply because class `1` dominates the dataset. However, this does **not** indicate that the classifier is actually effective‚Äîit merely reflects the class imbalance.\n","\n","\n","To better assess performance in the presence of class imbalance, we consider several standard classification metrics. In this context:\n","\n","- Class **1** is treated as the **positive class**  \n","- Class **0** is treated as the **negative class**\n","\n","\n","<center>\n","<img src=\"https://drive.google.com/uc?export=view&id=1SuL3Apx6Vx3tml5qC8H6MNYVPWFVUlOc\" width=600 />\n","</center>\n","\n","\n","\n","Here are the key metrics:\n","\n","- **Accuracy**:  \n","  The proportion of correctly classified instances out of all samples.  \n","  $$\n","  \\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Samples}}\n","  $$  \n","  ‚ö†Ô∏è Accuracy is **not reliable** in imbalanced datasets, as a model can achieve high accuracy by always predicting the majority class.\n","\n","- **Precision** (for class 1):  \n","  The proportion of predicted class `1` instances that are actually correct.  \n","  $$\n","  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n","  $$\n","\n","- **Recall** (for class 1):  \n","  The proportion of actual class `1` instances that are correctly predicted.  \n","  $$\n","  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n","  $$\n","\n","- **F1 Score** (for class 1):  \n","  The harmonic mean of precision and recall.  \n","  $$\n","  \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n","  $$\n","\n","üü¢ **Precision**, **Recall**, and **F1 Score** are more meaningful than accuracy in imbalanced settings. However, these metrics are usually reported when **the minority class is 1** by default.\n","\n","Since in our task **class `0` is the underrepresented class**, we flip the perspective and evaluate the metrics **with respect to class `0`**. In `scikit-learn`, this can be done as follows:\n","\n","\n","```python\n","from sklearn.metrics import f1_score\n","f1_score(y_test, y_pred, pos_label=0)\n","```\n","\n","**Another important consideration is how we split the dataset into training and test sets. It is essential that both sets reflect the same class distribution as the full dataset. This ensures that performance metrics are not skewed by an uneven class ratio in either the training or test split.**\n"],"metadata":{"id":"m8NOKxBVt6jG"}},{"cell_type":"code","source":["# We look at different metrics, because accuracy can be bad with unbalanced data\n","# If you don't know any of metrics below, google/ask GPT/look documentation\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score,\n","    f1_score, classification_report\n",")\n","\n","# Split the data into training and testing sets with stratified sampling\n","\n","X_train, X_test, y_train, y_test = #<YOUR CODE HERE> Don't forget to stratify!\n","\n","\n","# Train and evaluate polynomial logistic regression models for degrees 1 to 4\n","degrees = range(1, 5)\n","accuracies = []\n","f1_scores = []\n","precisions = []\n","recalls = []\n","\n","for degree in degrees:\n","    model = #<YOUR CODE HERE>\n","\n","    #<YOUR CODE HERE>\n","\n","\n","    # Evaluate metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred, pos_label=0)\n","    precision = precision_score(y_test, y_pred, pos_label=0)\n","    recall = recall_score(y_test, y_pred, pos_label=0)\n","\n","    accuracies.append(accuracy)\n","    f1_scores.append(f1)\n","    precisions.append(precision)\n","    recalls.append(recall)\n","\n","    print(f\"\\nDegree {degree} classification report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","# Plot\n","plt.plot(degrees, accuracies, marker='o', label='Accuracy')\n","plt.plot(degrees, f1_scores, marker='s', label='F1 Score')\n","plt.plot(degrees, precisions, marker='^', label='Precision')\n","plt.plot(degrees, recalls, marker='v', label='Recall')\n","plt.xlabel('Polynomial Degree')\n","plt.ylabel('Score')\n","plt.title('Model Evaluation Metrics')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"LvlT--zs9XtT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# best_model = #<YOUR CODE HERE>\n","y_pred = best_model.predict(X_test)\n","\n","def plot_pie_charts(y_true, y_pred):\n","    labels = np.unique(y_true)\n","\n","    fig, axes = plt.subplots(1, len(labels), figsize=(18, 6)) # Adjust figure size as needed\n","\n","    for i, label in enumerate(labels):\n","        true_positives = np.sum([(y_true[j] == label) & (y_pred[j] == label) for j in range(len(y_true))])\n","        total_elements = np.sum([y_true[j] == label for j in range(len(y_true))])\n","\n","        proportions = [true_positives, total_elements - true_positives]\n","\n","        axes[i].pie(proportions, labels=['True', 'False'], autopct='%1.1f%%', startangle=90,\n","                    colors=['lightgreen', 'lightcoral'])\n","        axes[i].set_title(f\"Class {label}\")\n","\n","    plt.show()\n","\n","plot_pie_charts(y_test, y_pred)\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"id":"3ixgNwYFM6zN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**We expect your model's f1 score to be higher than 0.97.**"],"metadata":{"id":"J4BK6eqYs40u"}},{"cell_type":"markdown","source":["Write a code that will help to identify which features (including polynomial combinations) have the most significant impact on the model by examining the learned weights of the logistic regression step. Output features sorted by the absolute value of their weights.\n","\n","If you use make_pipeline, you will need to acceess logistic regression part of it.\n","\n","\n"],"metadata":{"id":"Giuw-KdSn62-"}},{"cell_type":"code","source":["weights = dict() # Example: {\"a^1\": 1.0, \"b^1\":5.0, \"a^3c^1\":0.5}\n","#<YOUR CODE HERE>"],"metadata":{"id":"2emol0cHP5it"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explain the weights you see:\n","YOUR TEXT HERE"],"metadata":{"id":"F2w7zcJNxVsJ"}},{"cell_type":"markdown","source":["# Question $2^*$ (1 Point)\n","\n","What would happen if we tried to train a classifier on randomly sampled real-valued matrices?\n","\n","By \"randomly sampled,\" we mean that each element of the matrix is drawn independently from a uniform distribution.\n","\n","Answer: YOUR TEXT HERE"],"metadata":{"id":"sOb06Pd_oGPE"}}]}