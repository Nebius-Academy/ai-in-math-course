{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MKCsb_vgqKlnADy9sYCiBKZWgFBiDyPv","timestamp":1750945101355}],"collapsed_sections":["1MxcM467TvFh","QQBMjOeGT_zs","Y893frDeU0Kb"],"authorship_tag":"ABX9TyNBEa7B+UHw1ddxNyUJUXVh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HW8: Monte Carlo Tree Search (MCTS) for 3Ã—3 Tic-Tac-Toe\n","\n","In this notebook, we explore how to build intelligent agents to play a simple 3Ã—3 game with a win condition of 3 in a row. We'll test multiple agent strategies, including:\n","\n","- Heuristics\n","- An MCTS (Monte Carlo Tree Search) agent guided by a neural network\n","\n","We begin by implementing the core of the MCTS algorithm."],"metadata":{"id":"CcYpeOsXdIbD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZT1541oRqTNs"},"outputs":[],"source":["import os\n","import math\n","import numpy as np\n","from random import shuffle\n","import torch\n","import torch.optim as optim\n","from abc import ABC, abstractmethod\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import defaultdict, Counter\n","import pandas as pd\n","from typing import Dict, List, Tuple, Optional\n","import json\n","import random\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","source":["## ðŸ” Whatâ€™s in this cell?\n","\n","The following code defines two key components:\n","- `Node`: a class representing nodes in the MCTS search tree\n","- `MCTS`: a class that performs the MCTS algorithm, which repeatedly simulates games and backs up evaluations to guide better decisions\n","\n","The algorithm uses a neural network model for policy priors and state evaluation, and selects actions based on a UCB (Upper Confidence Bound) score.\n","\n","We will use this implementation throughout the notebook to build a powerful learning-based agent."],"metadata":{"id":"0cer2SwrUYE1"}},{"cell_type":"code","source":["def ucb_score(parent, child):\n","    \"\"\"\n","    The score for an action that would transition between the parent and child.\n","    \"\"\"\n","    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n","    if child.visit_count > 0:\n","        # The value of the child is from the perspective of the opposing player\n","        value_score = -child.value()\n","    else:\n","        value_score = 0\n","\n","    return value_score + prior_score\n","\n","\n","class Node:\n","    def __init__(self, prior, to_play):\n","        self.visit_count = 0\n","        self.to_play = to_play\n","        self.prior = prior\n","        self.value_sum = 0\n","        self.children = {}\n","        self.state = None\n","\n","    def expanded(self):\n","        return len(self.children) > 0\n","\n","    def value(self):\n","        if self.visit_count == 0:\n","            return 0\n","        return self.value_sum / self.visit_count\n","\n","    def select_action(self, temperature):\n","        \"\"\"\n","        Select action according to the visit count distribution and the temperature.\n","        \"\"\"\n","        visit_counts = np.array([child.visit_count for child in self.children.values()])\n","        actions = [action for action in self.children.keys()]\n","        if temperature == 0:\n","            action = actions[np.argmax(visit_counts)]\n","        elif temperature == float(\"inf\"):\n","            action = np.random.choice(actions)\n","        else:\n","            # See paper appendix Data Generation\n","            visit_count_distribution = visit_counts ** (1 / temperature)\n","            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n","            action = np.random.choice(actions, p=visit_count_distribution)\n","\n","        return action\n","\n","    def select_child(self):\n","        \"\"\"\n","        Select the child with the highest UCB score.\n","        \"\"\"\n","        best_score = -np.inf\n","        best_action = -1\n","        best_child = None\n","\n","        for action, child in self.children.items():\n","            score = ucb_score(self, child)\n","            if score > best_score:\n","                best_score = score\n","                best_action = action\n","                best_child = child\n","\n","        return best_action, best_child\n","\n","    def expand(self, state, to_play, action_probs):\n","        \"\"\"\n","        We expand a node and keep track of the prior policy probability given by neural network\n","        \"\"\"\n","        self.to_play = to_play\n","        self.state = state\n","        for a, prob in enumerate(action_probs):\n","            if prob != 0:\n","                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Debugger pretty print node info\n","        \"\"\"\n","        prior = \"{0:.2f}\".format(self.prior)\n","        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n","\n","\n","class MCTS:\n","\n","    def __init__(self, game, model, args):\n","        self.game = game\n","        self.model = model\n","        self.args = args\n","\n","    def run(self, model, state, to_play):\n","\n","        root = Node(0, to_play)\n","\n","        # EXPAND root\n","        action_probs, value = model.predict(state)\n","        valid_moves = self.game.get_valid_moves(state)\n","        action_probs = action_probs * valid_moves  # mask invalid moves\n","        action_probs /= np.sum(action_probs)\n","        root.expand(state, to_play, action_probs)\n","\n","        for _ in range(self.args['num_simulations']):\n","            node = root\n","            search_path = [node]\n","\n","            # SELECT\n","            while node.expanded():\n","                action, node = node.select_child()\n","                search_path.append(node)\n","\n","            parent = search_path[-2]\n","            state = parent.state\n","            # Now we're at a leaf node and we would like to expand\n","            # Players always play from their own perspective\n","            next_state, _ = self.game.get_next_state(state, player=1, action=action)\n","            # Get the board from the perspective of the other player\n","            next_state = self.game.get_canonical_board(next_state, player=-1)\n","\n","            # The value of the new state from the perspective of the other player\n","            value = self.game.get_reward_for_player(next_state, player=1)\n","            if value is None:\n","                # If the game has not ended:\n","                # EXPAND\n","                action_probs, value = model.predict(next_state)\n","                valid_moves = self.game.get_valid_moves(next_state)\n","                action_probs = action_probs * valid_moves  # mask invalid moves\n","                action_probs /= np.sum(action_probs)\n","                node.expand(next_state, parent.to_play * -1, action_probs)\n","\n","            self.backpropagate(search_path, value, parent.to_play * -1)\n","\n","        return root\n","\n","    def backpropagate(self, search_path, value, to_play):\n","        \"\"\"\n","        At the end of a simulation, we propagate the evaluation all the way up the tree\n","        to the root.\n","        \"\"\"\n","        for node in reversed(search_path):\n","            node.value_sum += value if node.to_play == to_play else -value\n","            node.visit_count += 1"],"metadata":{"id":"VD9mq1Fe8ACJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ðŸŽ® Game Environment: Generalized Tic-Tac-Toe (K-in-a-Row)\n","\n","To experiment with intelligent agents, we define a flexible game environment: a 2D grid where the goal is to connect **K pieces in a row** (horizontally, vertically, or diagonally).\n","\n","This implementation supports:\n","- Custom board size (e.g., 3Ã—3, 4Ã—4)\n","- Custom win condition (e.g., 3 in a row, 4 in a row)\n","- All necessary game mechanics: state transition, valid moves, win detection, and canonical board representation.\n","\n","We'll use this class to simulate our 3Ã—3 game with a win condition of 3."],"metadata":{"id":"QLEyv7EpR31q"}},{"cell_type":"code","source":["class TicTacToeK:\n","    \"\"\"\n","    Generalized Connect-K on a 2D grid.\n","    Default: 2x2 board, win condition = 2 in a row\n","    \"\"\"\n","\n","    def __init__(self, rows=2, cols=2, win=2):\n","        self.rows = rows\n","        self.cols = cols\n","        self.win = win\n","\n","    def get_init_board(self):\n","        return np.zeros((self.rows, self.cols), dtype=int)\n","\n","    def get_board_size(self):\n","        return (self.rows, self.cols)\n","\n","    def get_action_size(self):\n","        return self.rows * self.cols\n","\n","    def get_next_state(self, board, player, action):\n","        b = np.copy(board)\n","        r, c = divmod(action, self.cols)\n","        b[r, c] = player\n","        return b, -player\n","\n","    def has_legal_moves(self, board):\n","        return np.any(board == 0)\n","\n","    def get_valid_moves(self, board):\n","        return [(1 if board[r, c] == 0 else 0) for r in range(self.rows) for c in range(self.cols)]\n","\n","    def is_win(self, board, player):\n","        # Check rows\n","        for r in range(self.rows):\n","            for c in range(self.cols - self.win + 1):\n","                if all(board[r, c + i] == player for i in range(self.win)):\n","                    return True\n","        # Check columns\n","        for c in range(self.cols):\n","            for r in range(self.rows - self.win + 1):\n","                if all(board[r + i, c] == player for i in range(self.win)):\n","                    return True\n","        # Check diagonals\n","        for r in range(self.rows - self.win + 1):\n","            for c in range(self.cols - self.win + 1):\n","                if all(board[r + i, c + i] == player for i in range(self.win)):\n","                    return True\n","        # Check anti-diagonals\n","        for r in range(self.rows - self.win + 1):\n","            for c in range(self.win - 1, self.cols):\n","                if all(board[r + i, c - i] == player for i in range(self.win)):\n","                    return True\n","        return False\n","\n","    def get_reward_for_player(self, board, player):\n","        if self.is_win(board, player):\n","            return 1\n","        if self.is_win(board, -player):\n","            return -1\n","        if self.has_legal_moves(board):\n","            return None\n","        return 0  # draw\n","\n","    def get_canonical_board(self, board, player):\n","        return player * board"],"metadata":{"id":"kkN4yZhytYpr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Agent Implementations and Exploration Task\n","\n","In this cell, we define a set of agent classes, each representing a different strategy for playing the game. All agents inherit from the `BasePlayer` interface, which requires two methods:\n","\n","- `get_action`: returns the selected move given the current game state and player.\n","- `get_action_probs`: returns a probability distribution over actions (used by MCTS).\n","\n","\n","---\n","\n","### âœï¸ Your Task (3 points)\n","\n","Carefully read the implementations of the four agents above. For each agent:\n","\n","1. Explain **in your own words** how the agent makes decisions.\n","2. What are the advantages and limitations of each strategy?\n","3. Which agent do you expect to perform best in a 3Ã—3 Connect-3 game, and why?\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"LGq4ckE3TJQC"}},{"cell_type":"code","source":["\n","\n","class BasePlayer(ABC):\n","    \"\"\"Abstract base class for all players\"\"\"\n","\n","    @abstractmethod\n","    def get_action(self, game, state, player):\n","        \"\"\"Get action for the given state from this player's perspective\"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def get_action_probs(self, game, state, player):\n","        \"\"\"Get action probabilities for the given state\"\"\"\n","        pass\n","\n","\n","class MCTSPlayer(BasePlayer):\n","\n","    def __init__(self, model, args):\n","        self.model = model\n","        self.args = args\n","        self.mcts = None\n","\n","    def get_action(self, game, state, player):\n","        canonical_board = game.get_canonical_board(state, player)\n","        self.mcts = MCTS(game, self.model, self.args)\n","        root = self.mcts.run(self.model, canonical_board, to_play=1)\n","        return root.select_action(temperature=0)\n","\n","    def get_action_probs(self, game, state, player):\n","        canonical_board = game.get_canonical_board(state, player)\n","        self.mcts = MCTS(game, self.model, self.args)\n","        root = self.mcts.run(self.model, canonical_board, to_play=1)\n","\n","        action_probs = [0 for _ in range(game.get_action_size())]\n","        for k, v in root.children.items():\n","            action_probs[k] = v.visit_count\n","\n","        if np.sum(action_probs) > 0:\n","            action_probs = np.array(action_probs) / np.sum(action_probs)\n","        else:\n","            action_probs = np.array(action_probs)\n","\n","        return action_probs, root\n","\n","\n","class Player1(BasePlayer):\n","\n","    def get_action(self, game, state, player):\n","        valid_moves = game.get_valid_moves(game.get_canonical_board(state, player))\n","        valid_actions = [i for i, valid in enumerate(valid_moves) if valid]\n","        return np.random.choice(valid_actions)\n","\n","    def get_action_probs(self, game, state, player):\n","        valid_moves = game.get_valid_moves(game.get_canonical_board(state, player))\n","        action_probs = valid_moves / np.sum(valid_moves) if np.sum(valid_moves) > 0 else valid_moves\n","        return action_probs, None\n","\n","\n","class Player2(BasePlayer):\n","\n","    def get_action(self, game, state, player):\n","        canonical_board = game.get_canonical_board(state, player)\n","        valid_moves = game.get_valid_moves(canonical_board)\n","        valid_actions = [i for i, valid in enumerate(valid_moves) if valid]\n","\n","        best_action = valid_actions[0]\n","        best_reward = float('-inf')\n","\n","        for action in valid_actions:\n","            next_state, _ = game.get_next_state(state, player, action)\n","            reward = game.get_reward_for_player(next_state, player)\n","            if reward is not None and reward > best_reward:\n","                best_reward = reward\n","                best_action = action\n","\n","        return best_action\n","\n","    def get_action_probs(self, game, state, player):\n","        canonical_board = game.get_canonical_board(state, player)\n","        valid_moves = game.get_valid_moves(canonical_board)\n","        valid_actions = [i for i, valid in enumerate(valid_moves) if valid]\n","\n","        action_probs = np.zeros(game.get_action_size())\n","        best_action = self.get_action(game, state, player)\n","        action_probs[best_action] = 1.0\n","\n","        return action_probs, None\n","\n","class Player3(BasePlayer):\n","\n","    def __init__(self, win_size):\n","        self.win = win_size\n","\n","    def get_action(self, game, state, player):\n","        b = game.get_canonical_board(state, player)\n","        # ensure 2D for uniform windowing\n","        mat = b if b.ndim==2 else b.reshape(1, -1)\n","        rows, cols = mat.shape\n","        valid = game.get_valid_moves(b)\n","        actions = {i for i,v in enumerate(valid) if v}\n","\n","        def windows():\n","            # yield list of (r,c) coords for each length-win window in all directions\n","            dirs = [(0,1),(1,0),(1,1),(1,-1)]\n","            for dr,dc in dirs:\n","                for r in range(rows):\n","                    for c in range(cols):\n","                        coords = [(r+i*dr, c+i*dc) for i in range(self.win)]\n","                        if all(0<=rr<rows and 0<=cc<cols for rr,cc in coords):\n","                            yield coords\n","\n","        opp = -1; me = 1\n","\n","        # 1) WIN_SIZE-2 threats\n","        threats2 = []\n","        for coords in windows():\n","            vals = [mat[r,c] for r,c in coords]\n","            if vals.count(opp)==self.win-2 and vals.count(0)>=2:\n","                empties = [(r,c) for r,c in coords if mat[r,c]==0]\n","                threats2.append(empties)\n","        if threats2:\n","            empties = random.choice(threats2)\n","            r,c = random.choice(empties)\n","            a = r*cols + c if b.ndim>1 else c\n","            if a in actions: return a\n","\n","        # 2) WIN_SIZE-1 threats\n","        blocks1 = []\n","        for coords in windows():\n","            vals = [mat[r,c] for r,c in coords]\n","            if vals.count(opp)==self.win-1 and vals.count(0)>=1:\n","                empties = [(r,c) for r,c in coords if mat[r,c]==0]\n","                blocks1.extend(empties)\n","        if blocks1:\n","            r,c = random.choice(blocks1)\n","            a = r*cols + c if b.ndim>1 else c\n","            if a in actions: return a\n","\n","        # 3)\n","        my_counts = []\n","        win_windows = list(windows())\n","        for coords in win_windows:\n","            vals = [mat[r,c] for r,c in coords]\n","            my_counts.append(vals.count(me))\n","        max_me = max(my_counts) if my_counts else 0\n","\n","        extend_cands = []\n","        for coords, cnt in zip(win_windows, my_counts):\n","            if cnt==max_me and cnt>0:\n","                empties = [(r,c) for r,c in coords if mat[r,c]==0]\n","                extend_cands.extend(empties)\n","        if extend_cands:\n","            r,c = random.choice(extend_cands)\n","            a = r*cols + c if b.ndim>1 else c\n","            if a in actions: return a\n","\n","        # 4) fallback random\n","        return random.choice(list(actions))\n","\n","    def get_action_probs(self, game, state, player):\n","        a = self.get_action(game, state, player)\n","        probs = np.zeros(game.get_action_size())\n","        probs[a] = 1.0\n","        return probs, None\n","\n","\n","\n","\n"],"metadata":{"id":"ov04YCUrqWCr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Against a Fixed Opponent\n","\n","The `FixedOpponentTrainer` class provides functionality to train a neural network policy/value model against a fixed opponent. This setup simulates a player that continually improves while playing against a static agent.\n","\n","The training procedure consists of the following components:\n","\n","- **Self-play episodes**: The trainable agent (using MCTS) plays full games against the fixed opponent. During the trainable player's turn, it collects training examples consisting of:\n","  - The canonical board state.\n","  - The action probabilities from MCTS.\n","  - The final game outcome (used as the value target).\n","\n","- **Reward assignment**: After the game ends, each move is labeled with a reward (win/loss/draw), adjusted according to which player took the action.\n","\n","- **Neural network updates**: Collected examples are used to update the trainable model's policy and value heads using standard loss functions (cross-entropy for policy, MSE for value).\n","\n"],"metadata":{"id":"1MxcM467TvFh"}},{"cell_type":"code","source":["class FixedOpponentTrainer:\n","    \"\"\"Trainer that allows training against a fixed opponent\"\"\"\n","\n","    def __init__(self, game, trainable_model, fixed_player, args, trainable_player=1):\n","        \"\"\"\n","        Args:\n","            game: Game instance\n","            trainable_model: Model to be trained\n","            fixed_player: Instance of BasePlayer for the fixed opponent\n","            args: Training arguments\n","            trainable_player: Which player to train (1 or -1)\n","        \"\"\"\n","        self.game = game\n","        self.trainable_model = trainable_model\n","        self.fixed_player = fixed_player\n","        self.args = args\n","        self.trainable_player = trainable_player\n","        self.trainable_mcts_player = MCTSPlayer(trainable_model, args)\n","\n","    def execute_episode(self):\n","        \"\"\"Execute one episode of self-play against fixed opponent\"\"\"\n","        train_examples = []\n","        current_player = 1\n","        state = self.game.get_init_board()\n","\n","        while True:\n","            if current_player == self.trainable_player:\n","                # Trainable player's turn - collect training data\n","                canonical_board = self.game.get_canonical_board(state, current_player)\n","                action_probs, root = self.trainable_mcts_player.get_action_probs(self.game, state, current_player)\n","                train_examples.append((canonical_board, current_player, action_probs))\n","                action = root.select_action(temperature=0)\n","            else:\n","                # Fixed player's turn - no training data collected\n","                action = self.fixed_player.get_action(self.game, state, current_player)\n","\n","            state, current_player = self.game.get_next_state(state, current_player, action)\n","            reward = self.game.get_reward_for_player(state, current_player)\n","\n","            if reward is not None:\n","                # Game ended, assign rewards\n","                ret = []\n","                for hist_state, hist_current_player, hist_action_probs in train_examples:\n","                    # Reward from the perspective of the trainable player\n","                    if self.trainable_player == 1:\n","                        final_reward = reward if current_player == 1 else -reward\n","                    else:\n","                        final_reward = reward if current_player == -1 else -reward\n","\n","                    # Adjust reward based on which player made the move\n","                    player_reward = final_reward * ((-1) ** (hist_current_player != self.trainable_player))\n","                    ret.append((hist_state, hist_action_probs, player_reward))\n","\n","                return ret\n","\n","    def learn(self):\n","        \"\"\"Main training loop\"\"\"\n","        for i in range(1, self.args['numIters'] + 1):\n","            print(f\"Iteration {i}/{self.args['numIters']}\")\n","\n","            train_examples = []\n","\n","            for eps in range(self.args['numEps']):\n","                iteration_train_examples = self.execute_episode()\n","                train_examples.extend(iteration_train_examples)\n","\n","            shuffle(train_examples)\n","            self.train(train_examples)\n","\n","\n","\n","    def train(self, examples):\n","        \"\"\"Train the neural network\"\"\"\n","        optimizer = optim.Adam(self.trainable_model.parameters(), lr=5e-4)\n","        pi_losses = []\n","        v_losses = []\n","\n","        for epoch in range(self.args['epochs']):\n","            self.trainable_model.train()\n","            batch_idx = 0\n","\n","            while batch_idx < int(len(examples) / self.args['batch_size']):\n","                sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n","                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n","                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n","                target_pis = torch.FloatTensor(np.array(pis))\n","                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n","\n","                # Move to device if available\n","                if torch.cuda.is_available():\n","                    device = torch.device('cuda')\n","                    boards = boards.contiguous().to(device)\n","                    target_pis = target_pis.contiguous().to(device)\n","                    target_vs = target_vs.contiguous().to(device)\n","\n","                # Compute output\n","                out_pi, out_v = self.trainable_model(boards)\n","                l_pi = self.loss_pi(target_pis, out_pi)\n","                l_v = self.loss_v(target_vs, out_v)\n","                total_loss = l_pi + l_v\n","\n","                pi_losses.append(float(l_pi))\n","                v_losses.append(float(l_v))\n","\n","                optimizer.zero_grad()\n","                total_loss.backward()\n","                optimizer.step()\n","\n","                batch_idx += 1\n","\n","\n","    def loss_pi(self, targets, outputs):\n","        \"\"\"Policy loss function\"\"\"\n","        loss = -(targets * torch.log(outputs + 1e-8)).sum(dim=1)\n","        return loss.mean()\n","\n","    def loss_v(self, targets, outputs):\n","        \"\"\"Value loss function\"\"\"\n","        loss = torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n","        return loss"],"metadata":{"id":"xebwNKuoSo5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Neural Network Model for Connect Game\n","\n","The `ConnectPolicyValueNet` class defines a simple feedforward neural network used to approximate both the **policy** (action probabilities) and **value** (expected game outcome) for a given game board.\n","\n","- The input is a flattened board state.\n","- It outputs:\n","  - A **softmax** probability vector over all legal actions (policy head).\n","  - A scalar value between **âˆ’1** and **1** representing the expected outcome for the current player (value head).\n","\n","This model is used by the MCTS algorithm to guide simulations and evaluate board positions during training and play.\n","\n","---\n","\n","### ðŸ§  Task (2 points)\n","Implement your own version of `ConnectPolicyValueNet`:\n","- Use at least **two hidden layers**.\n","- Use **ReLU** activations and appropriate final activations (`softmax` for policy, `tanh` for value).\n","- Your class should define a `forward` method and a `predict(board)` method that returns both policy and value outputs.\n","\n","```python\n","# Your code here:\n","class ConnectPolicyValueNet(nn.Module):\n","    ...\n","```"],"metadata":{"id":"QQBMjOeGT_zs"}},{"cell_type":"code","source":["class ConnectPolicyValueNet(nn.Module):\n","    # ..."],"metadata":{"id":"9cShNIEb7bqR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training a Policy-Value Network on 3Ã—3 Tic-Tac-Toe\n","\n","We now set up and train our agent to play a generalized 3Ã—3 Tic-Tac-Toe game with a win condition of 3 in a row.\n","\n","#### Key Components:\n","- **Game Setup**:\n","  - The board is 3Ã—3 and the player must align 3 pieces to win.\n","- **Model**:\n","  - We instantiate our neural network (`ConnectPolicyValueNet`) with appropriate input/output dimensions.\n","- **Training Arguments**:\n","  - `numIters`: number of training iterations.\n","  - `numEps`: number of self-play episodes per iteration.\n","  - `num_simulations`: number of MCTS simulations per move.\n","  - `epochs`, `batch_size`: used to train the model from generated data.\n","\n","#### Opponent:\n","The `FixedOpponentTrainer` class orchestrates self-play between the trainable MCTS agent and the fixed opponent, collecting training data and updating the model accordingly.\n","\n","---\n","\n","### âœ… Your Task: (5 points)\n","\n","- Try training your model against different opponent agents (e.g., `RandomPlayer`, `SequenceThreatPlayer`, etc.).\n","- Experiment with different training configurations by tuning the values in the `args` dictionary.\n","\n","Below is a list of the key hyperparameters. After trying different values, **explain in your own words** what each parameter controls and how it influences learning.\n","\n","ðŸ” **Your goal** is to find a configuration that trains a model which performs well against provided opponents. Reflect on which hyperparameters were most impactful and why.\n","\n","We expect your model to win 80 percent of the games against our players."],"metadata":{"id":"usncL8ZqVoh-"}},{"cell_type":"code","source":["BOARD_ROWS = 3\n","BOARD_COLS = 3\n","WIN_SIZE = 3\n","\n","game = TicTacToeK(rows=BOARD_ROWS, cols=BOARD_COLS, win=WIN_SIZE)\n","board_size = game.get_board_size()\n","action_size = game.get_action_size()\n","\n","model = ConnectPolicyValueNet(board_size, action_size, device)\n","\n","args = {\n","    'numIters': 1,\n","    'numEps': 1,\n","    'num_simulations': 1,\n","    'epochs': 1,\n","    'batch_size': 64\n","}\n","\n","# Choose a fixed opponent\n","fixed_opponent = #...\n","# Create trainer\n","trainer = FixedOpponentTrainer(\n","    game=game,\n","    trainable_model=model,\n","    fixed_player=fixed_opponent,\n","    args=args,\n","    trainable_player=  # ...\n",")\n","\n","# Start training\n","trainer.learn()\n"],"metadata":{"id":"wy0Nxwqu7JHk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Class to calculate performance"],"metadata":{"id":"Y893frDeU0Kb"}},{"cell_type":"code","source":["\n","\n","class GameStats:\n","    \"\"\"Class to track game statistics\"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.wins = 0\n","        self.losses = 0\n","        self.draws = 0\n","        self.total_games = 0\n","        self.total_moves = 0\n","        self.game_lengths = []\n","        self.move_times = []\n","        self.rewards = []\n","\n","    def add_game_result(self, result, game_length, avg_move_time, final_reward):\n","        \"\"\"\n","        Add result of a single game\n","        result: 1 for win, -1 for loss, 0 for draw\n","        \"\"\"\n","        if result == 1:\n","            self.wins += 1\n","        elif result == -1:\n","            self.losses += 1\n","        else:\n","            self.draws += 1\n","\n","        self.total_games += 1\n","        self.total_moves += game_length\n","        self.game_lengths.append(game_length)\n","        self.move_times.append(avg_move_time)\n","        self.rewards.append(final_reward)\n","\n","    def get_win_rate(self):\n","        return self.wins / self.total_games if self.total_games > 0 else 0\n","\n","    def get_stats_dict(self):\n","        return {\n","            'wins': self.wins,\n","            'losses': self.losses,\n","            'draws': self.draws,\n","            'total_games': self.total_games,\n","            'win_rate': self.get_win_rate(),\n","            'avg_game_length': np.mean(self.game_lengths) if self.game_lengths else 0,\n","            'avg_move_time': np.mean(self.move_times) if self.move_times else 0,\n","            'avg_reward': np.mean(self.rewards) if self.rewards else 0\n","        }\n","\n","\n","class ModelTester:\n","    \"\"\"Comprehensive testing system for game models\"\"\"\n","\n","    def __init__(self, game, model, args):\n","        self.game = game\n","        self.model = model\n","        self.args = args\n","        self.test_results = {}\n","        self.sample_games = {}  # Store sample games for each opponent\n","\n","    def play_single_game(self, player1, player2, verbose=False):\n","        \"\"\"\n","        Play a single game between two players\n","        Returns: (winner, game_length, move_times, final_state, game_trace)\n","        \"\"\"\n","        state = self.game.get_init_board()\n","        current_player = 1\n","        move_count = 0\n","        move_times = []\n","        game_trace = []  # Store game moves for replay\n","\n","        if verbose:\n","            print(\"Starting new game...\")\n","\n","        while True:\n","            start_time = time.time()\n","\n","            if current_player == 1:\n","                action = player1.get_action(self.game, state, current_player)\n","            else:\n","                action = player2.get_action(self.game, state, current_player)\n","\n","            move_time = time.time() - start_time\n","            move_times.append(move_time)\n","            move_count += 1\n","\n","            # Store move in trace\n","            game_trace.append({\n","                'move': move_count,\n","                'player': current_player,\n","                'action': action,\n","                'state_before': state.copy() if hasattr(state, 'copy') else str(state),\n","                'move_time': move_time\n","            })\n","\n","            if verbose:\n","                print(f\"Player {current_player} chose action {action} in {move_time:.3f}s\")\n","\n","            state, current_player = self.game.get_next_state(state, current_player, action)\n","            reward = self.game.get_reward_for_player(state, current_player)\n","\n","            if reward is not None:\n","                winner = current_player if reward > 0 else (-current_player if reward < 0 else 0)\n","                game_trace.append({\n","                    'final_state': state.copy() if hasattr(state, 'copy') else str(state),\n","                    'winner': winner\n","                })\n","                if verbose:\n","                    print(f\"Game ended. Winner: {winner if winner != 0 else 'Draw'}\")\n","                return winner, move_count, move_times, state, game_trace\n","\n","    def test_against_opponent(self, opponent, num_games=100, test_as_both_players=True, verbose=False):\n","        \"\"\"\n","        Test model against a specific opponent\n","        \"\"\"\n","        print(f\"Testing against {opponent.__class__.__name__} ({num_games} games)...\")\n","\n","        model_player = MCTSPlayer(self.model, self.args)\n","        results = {}\n","        sample_game_trace = None\n","\n","        if test_as_both_players:\n","            # Test as player 1\n","            stats_as_p1 = GameStats()\n","            for i in range(num_games // 2):\n","                if verbose or i % 10 == 0:\n","                    print(f\"Game {i+1}/{num_games//2} (as Player 1)\")\n","\n","                winner, length, times, _, trace = self.play_single_game(model_player, opponent, verbose)\n","                result = 1 if winner == 1 else (-1 if winner == -1 else 0)\n","                stats_as_p1.add_game_result(result, length, np.mean(times), winner)\n","\n","                # Randomly select a game for sample replay\n","                if sample_game_trace is None and random.random() < 0.1:  # 10% chance\n","                    sample_game_trace = trace\n","\n","            # Test as player 2\n","            stats_as_p2 = GameStats()\n","            for i in range(num_games // 2):\n","                if verbose or i % 10 == 0:\n","                    print(f\"Game {i+1}/{num_games//2} (as Player 2)\")\n","\n","                winner, length, times, _, trace = self.play_single_game(opponent, model_player, verbose)\n","                result = 1 if winner == -1 else (-1 if winner == 1 else 0)  # Flip perspective\n","                stats_as_p2.add_game_result(result, length, np.mean(times), -winner if winner != 0 else 0)\n","\n","                # Randomly select a game for sample replay if we don't have one yet\n","                if sample_game_trace is None and random.random() < 0.1:\n","                    sample_game_trace = trace\n","\n","            results['as_player_1'] = stats_as_p1.get_stats_dict()\n","            results['as_player_2'] = stats_as_p2.get_stats_dict()\n","\n","            # Combined stats\n","            combined_stats = GameStats()\n","            combined_stats.wins = stats_as_p1.wins + stats_as_p2.wins\n","            combined_stats.losses = stats_as_p1.losses + stats_as_p2.losses\n","            combined_stats.draws = stats_as_p1.draws + stats_as_p2.draws\n","            combined_stats.total_games = stats_as_p1.total_games + stats_as_p2.total_games\n","            combined_stats.game_lengths = stats_as_p1.game_lengths + stats_as_p2.game_lengths\n","            combined_stats.move_times = stats_as_p1.move_times + stats_as_p2.move_times\n","            combined_stats.rewards = stats_as_p1.rewards + stats_as_p2.rewards\n","\n","            results['combined'] = combined_stats.get_stats_dict()\n","\n","        else:\n","            # Test only as player 1\n","            stats = GameStats()\n","            for i in range(num_games):\n","                if verbose or i % 10 == 0:\n","                    print(f\"Game {i+1}/{num_games}\")\n","\n","                winner, length, times, _, trace = self.play_single_game(model_player, opponent, verbose)\n","                result = 1 if winner == 1 else (-1 if winner == -1 else 0)\n","                stats.add_game_result(result, length, np.mean(times), winner)\n","\n","                # Randomly select a game for sample replay\n","                if sample_game_trace is None and random.random() < 0.1:\n","                    sample_game_trace = trace\n","\n","            results['combined'] = stats.get_stats_dict()\n","\n","        # Store the sample game trace\n","        if sample_game_trace:\n","            self.sample_games[opponent.__class__.__name__] = sample_game_trace\n","\n","        return results\n","\n","    def comprehensive_test(self, opponents_config, num_games_per_opponent=100, save_results=True):\n","        \"\"\"\n","        Run comprehensive tests against multiple opponents\n","\n","        opponents_config: dict with opponent_name -> opponent_instance\n","        \"\"\"\n","        print(\"Starting comprehensive model testing...\")\n","        print(\"=\" * 50)\n","\n","        all_results = {}\n","\n","        for opponent_name, opponent in opponents_config.items():\n","            results = self.test_against_opponent(\n","                opponent,\n","                num_games=num_games_per_opponent,\n","                test_as_both_players=True\n","            )\n","            all_results[opponent_name] = results\n","\n","            # Print summary\n","            combined = results['combined']\n","            print(f\"\\n{opponent_name} Results:\")\n","            print(f\"  Win Rate: {combined['win_rate']:.1%}\")\n","            print(f\"  Games: {combined['wins']}W - {combined['losses']}L - {combined['draws']}D\")\n","            print(f\"  Avg Game Length: {combined['avg_game_length']:.1f} moves\")\n","            print(f\"  Avg Move Time: {combined['avg_move_time']:.3f}s\")\n","\n","            if 'as_player_1' in results:\n","                p1_wr = results['as_player_1']['win_rate']\n","                p2_wr = results['as_player_2']['win_rate']\n","                print(f\"  As Player 1: {p1_wr:.1%} | As Player 2: {p2_wr:.1%}\")\n","\n","        self.test_results = all_results\n","\n","        # Print sample games for all opponents\n","        self.print_all_sample_games()\n","\n","        if save_results:\n","            self.save_results()\n","\n","        return all_results\n","\n","    def print_all_sample_games(self):\n","        \"\"\"Print sample games for all tested opponents\"\"\"\n","        if not self.sample_games:\n","            print(\"\\nNo sample games recorded.\")\n","            return\n","\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"SAMPLE GAMES\")\n","        print(\"=\" * 80)\n","\n","        for opponent_name, game_trace in self.sample_games.items():\n","            self.print_game_trace(game_trace, opponent_name)\n","\n","    def print_game_trace(self, game_trace, opponent_name):\n","        \"\"\"Print a detailed trace of a recorded game\"\"\"\n","        print(f\"\\n{'='*60}\")\n","        print(f\"SAMPLE GAME vs {opponent_name}\")\n","        print(f\"{'='*60}\")\n","\n","        if not game_trace:\n","            print(\"No game trace available\")\n","            return\n","\n","        # Extract moves and final result\n","        moves = [entry for entry in game_trace if 'move' in entry]\n","        final_entry = [entry for entry in game_trace if 'final_state' in entry]\n","\n","        if not moves:\n","            print(\"No moves recorded in trace\")\n","            return\n","\n","        print(f\"Game with {len(moves)} moves:\")\n","        print()\n","\n","        for move_info in moves:\n","            player = move_info['player']\n","            action = move_info['action']\n","            move_time = move_info['move_time']\n","            move_num = move_info['move']\n","\n","            player_name = \"MODEL\" if player == 1 else opponent_name\n","            print(f\"Move {move_num}: {player_name} (Player {player})\")\n","            print(f\"  Action: {action}\")\n","            print(f\"  Time: {move_time:.3f}s\")\n","            print(f\"  State before move:\")\n","            print(f\"  {move_info['state_before']}\")\n","            print()\n","\n","        if final_entry:\n","            winner = final_entry[0]['winner']\n","            print(f\"FINAL STATE:\")\n","            print(f\"{final_entry[0]['final_state']}\")\n","            print()\n","            print(f\"RESULT:\")\n","            if winner == 1:\n","                print(f\"ðŸŽ‰ MODEL WINS! ðŸŽ‰\")\n","            elif winner == -1:\n","                print(f\"ðŸ˜ž {opponent_name.upper()} WINS ðŸ˜ž\")\n","            else:\n","                print(f\"ðŸ¤ DRAW ðŸ¤\")\n","\n","        print(f\"{'='*60}\")\n","\n","    def analyze_playing_style(self, opponent, num_games=50):\n","        \"\"\"Analyze model's playing style and patterns\"\"\"\n","        print(f\"Analyzing playing style against {opponent.__class__.__name__}...\")\n","\n","        model_player = MCTSPlayer(self.model, self.args)\n","        action_frequency = Counter()\n","        game_lengths = []\n","        opening_moves = []\n","\n","        for i in range(num_games):\n","            state = self.game.get_init_board()\n","            current_player = 1\n","            move_count = 0\n","            first_move = None\n","\n","            while True:\n","                if current_player == 1:  # Model's turn\n","                    action = model_player.get_action(self.game, state, current_player)\n","                    action_frequency[action] += 1\n","\n","                    if first_move is None:\n","                        first_move = action\n","                else:\n","                    action = opponent.get_action(self.game, state, current_player)\n","\n","                move_count += 1\n","                state, current_player = self.game.get_next_state(state, current_player, action)\n","                reward = self.game.get_reward_for_player(state, current_player)\n","\n","                if reward is not None:\n","                    game_lengths.append(move_count)\n","                    opening_moves.append(first_move)\n","                    break\n","\n","        style_analysis = {\n","            'action_frequency': dict(action_frequency),\n","            'most_common_actions': action_frequency.most_common(5),\n","            'avg_game_length': np.mean(game_lengths),\n","            'opening_move_diversity': len(set(opening_moves)),\n","            'most_common_opening': Counter(opening_moves).most_common(1)[0] if opening_moves else None\n","        }\n","\n","        return style_analysis\n","\n","    def create_performance_report(self):\n","        \"\"\"Create a detailed performance report\"\"\"\n","        if not self.test_results:\n","            print(\"No test results available. Run tests first.\")\n","            return\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"COMPREHENSIVE PERFORMANCE REPORT\")\n","        print(\"=\" * 60)\n","\n","        # Overall performance summary\n","        all_win_rates = []\n","        for opponent, results in self.test_results.items():\n","            win_rate = results['combined']['win_rate']\n","            all_win_rates.append(win_rate)\n","\n","        print(f\"\\nOVERALL PERFORMANCE:\")\n","        print(f\"Average Win Rate: {np.mean(all_win_rates):.1%}\")\n","        print(f\"Best Performance: {np.max(all_win_rates):.1%}\")\n","        print(f\"Worst Performance: {np.min(all_win_rates):.1%}\")\n","        print(f\"Win Rate Standard Deviation: {np.std(all_win_rates):.1%}\")\n","\n","        # Detailed results per opponent\n","        print(f\"\\nDETAILED RESULTS:\")\n","        for opponent, results in self.test_results.items():\n","            combined = results['combined']\n","            print(f\"\\nvs {opponent}:\")\n","            print(f\"  Win Rate: {combined['win_rate']:.1%}\")\n","            print(f\"  Record: {combined['wins']}W-{combined['losses']}L-{combined['draws']}D\")\n","            print(f\"  Avg Game Length: {combined['avg_game_length']:.1f} moves\")\n","            print(f\"  Avg Move Time: {combined['avg_move_time']:.3f}s\")\n","\n","            if 'as_player_1' in results:\n","                p1_wr = results['as_player_1']['win_rate']\n","                p2_wr = results['as_player_2']['win_rate']\n","                print(f\"  As Player 1: {p1_wr:.1%} | As Player 2: {p2_wr:.1%}\")\n","\n","    def plot_results(self, save_plots=True):\n","        \"\"\"Create visualization plots of test results\"\"\"\n","        if not self.test_results:\n","            print(\"No test results to plot.\")\n","            return\n","\n","        # Prepare data\n","        opponents = list(self.test_results.keys())\n","        win_rates = [self.test_results[opp]['combined']['win_rate'] for opp in opponents]\n","        game_lengths = [self.test_results[opp]['combined']['avg_game_length'] for opp in opponents]\n","        move_times = [self.test_results[opp]['combined']['avg_move_time'] for opp in opponents]\n","\n","        # Prepare win rates by starting position\n","        p1_win_rates = []\n","        p2_win_rates = []\n","        for opp in opponents:\n","            if 'as_player_1' in self.test_results[opp]:\n","                p1_win_rates.append(self.test_results[opp]['as_player_1']['win_rate'])\n","                p2_win_rates.append(self.test_results[opp]['as_player_2']['win_rate'])\n","            else:\n","                p1_win_rates.append(win_rates[opponents.index(opp)])\n","                p2_win_rates.append(0)  # No data for player 2\n","\n","        # Create subplots\n","        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","        fig.suptitle('Model Performance Analysis', fontsize=16)\n","\n","        # Overall win rates bar chart\n","        axes[0, 0].bar(opponents, win_rates, color='skyblue')\n","        axes[0, 0].set_title('Overall Win Rates by Opponent')\n","        axes[0, 0].set_ylabel('Win Rate')\n","        axes[0, 0].set_ylim(0, 1)\n","        axes[0, 0].tick_params(axis='x', rotation=45)\n","        for i, v in enumerate(win_rates):\n","            axes[0, 0].text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')\n","\n","        # Win rates by starting position\n","        x = np.arange(len(opponents))\n","        width = 0.35\n","        axes[0, 1].bar(x - width/2, p1_win_rates, width, label='Starting First', color='lightgreen')\n","        axes[0, 1].bar(x + width/2, p2_win_rates, width, label='Starting Second', color='lightcoral')\n","        axes[0, 1].set_title('Win Rates by Starting Position')\n","        axes[0, 1].set_ylabel('Win Rate')\n","        axes[0, 1].set_ylim(0, 1)\n","        axes[0, 1].set_xticks(x)\n","        axes[0, 1].set_xticklabels(opponents, rotation=45)\n","        axes[0, 1].legend()\n","\n","        # Add value labels on bars\n","        for i, (v1, v2) in enumerate(zip(p1_win_rates, p2_win_rates)):\n","            if v1 > 0:\n","                axes[0, 1].text(i - width/2, v1 + 0.01, f'{v1:.1%}', ha='center', va='bottom', fontsize=8)\n","            if v2 > 0:\n","                axes[0, 1].text(i + width/2, v2 + 0.01, f'{v2:.1%}', ha='center', va='bottom', fontsize=8)\n","\n","        # Game lengths\n","        axes[1, 0].bar(opponents, game_lengths)\n","        axes[1, 0].set_title('Average Game Length by Opponent')\n","        axes[1, 0].set_ylabel('Moves per Game')\n","        axes[1, 0].tick_params(axis='x', rotation=45)\n","        for i, v in enumerate(game_lengths):\n","            axes[1, 0].text(i, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n","\n","        # Win rate vs game length scatter\n","        axes[1, 1].scatter(game_lengths, win_rates, s=100, alpha=0.7, color='purple')\n","        for i, opp in enumerate(opponents):\n","            axes[1, 1].annotate(opp, (game_lengths[i], win_rates[i]),\n","                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n","        axes[1, 1].set_xlabel('Average Game Length')\n","        axes[1, 1].set_ylabel('Win Rate')\n","        axes[1, 1].set_title('Win Rate vs Game Length')\n","        axes[1, 1].grid(True, alpha=0.3)\n","\n","        plt.tight_layout()\n","\n","        if save_plots:\n","            plt.savefig('model_performance_analysis.png', dpi=300, bbox_inches='tight')\n","            print(\"Plots saved as 'model_performance_analysis.png'\")\n","\n","        plt.show()\n","\n","    def save_results(self, filename='test_results.json'):\n","        \"\"\"Save test results to file\"\"\"\n","        results_to_save = {\n","            'test_results': self.test_results,\n","            'sample_games': self.sample_games\n","        }\n","        with open(filename, 'w') as f:\n","            json.dump(results_to_save, f, indent=2, default=str)  # default=str to handle numpy types\n","        print(f\"Results saved to {filename}\")\n","\n","    def load_results(self, filename='test_results.json'):\n","        \"\"\"Load test results from file\"\"\"\n","        try:\n","            with open(filename, 'r') as f:\n","                data = json.load(f)\n","            self.test_results = data.get('test_results', {})\n","            self.sample_games = data.get('sample_games', {})\n","            print(f\"Results loaded from {filename}\")\n","        except FileNotFoundError:\n","            print(f\"File {filename} not found.\")\n","\n","\n","# Import the player classes from the previous artifact\n","# (Assuming they are available in the same environment)\n","\n","def create_test_suite(game, model, args):\n","    \"\"\"Create a standard test suite with common opponents\"\"\"\n","\n","    # Create various opponents\n","    opponents = {\n","        'Player1': Player1(),\n","        'Player2': Player2(),\n","        'Player3': Player3(WIN_SIZE)\n","    }\n","\n","    # You can add more opponents like:\n","    # 'Pretrained_Model': MCTSPlayer(pretrained_model, args),\n","    # 'Weaker_MCTS': MCTSPlayer(model, {**args, 'num_simulations': 10}),\n","\n","    return opponents\n","\n","def play_against_agent(game, agent_player, human_first=True):\n","    \"\"\"\n","    Play a human vs any BasePlayer agent (MCTS, heuristic, etc.).\n","    \"\"\"\n","    state = game.get_init_board()\n","    current_player = -1 if human_first else 1\n","\n","    is_2d = len(state.shape) == 2\n","    rows, cols = state.shape if is_2d else (1, state.shape[0])\n","\n","    def display_board(board):\n","        symbols = {0: \".\", 1: \"X\", -1: \"O\"}\n","        print(\"\\nBoard:\")\n","        for r in range(rows):\n","            row = [symbols[board[r, c] if is_2d else board[c]] for c in range(cols)]\n","            print(\" \" + \" | \".join(row))\n","        print()\n","\n","    def get_move_from_human(valid):\n","        while True:\n","            try:\n","                if is_2d:\n","                    inp = input(\"Your move (row col): \").strip().split()\n","                    if len(inp) != 2:\n","                        raise ValueError\n","                    r, c = map(int, inp)\n","                    a = r * cols + c\n","                else:\n","                    a = int(input(f\"Your move (0 to {cols - 1}): \"))\n","                if 0 <= a < len(valid) and valid[a] == 1:\n","                    return a\n","            except:\n","                print(\"Invalid input. Try again.\")\n","\n","    def display_probs(probs):\n","        print(\"Agent move probabilities:\")\n","        for r in range(rows):\n","            row_probs = [f\"{probs[r * cols + c]:.2f}\" for c in range(cols)]\n","            print(\" \" + \" | \".join(row_probs))\n","        print()\n","\n","    print(\"Welcome! You're O. Agent is X.\")\n","    display_board(state)\n","\n","    while True:\n","        if current_player == -1:\n","            move = get_move_from_human(game.get_valid_moves(state))\n","        else:\n","            print(\"Agent is thinking...\")\n","            probs, _ = agent_player.get_action_probs(game, state, current_player)\n","            move = agent_player.get_action(game, state, current_player)\n","            display_probs(probs)\n","\n","        state, current_player = game.get_next_state(state, current_player, move)\n","        display_board(state)\n","\n","        reward = game.get_reward_for_player(state, current_player)\n","        if reward is not None:\n","            print(\"Game over!\")\n","            if reward == 0:\n","                print(\"It's a draw!\")\n","            else:\n","                print(\"You win!\" if current_player == 1 else \"Agent wins!\")\n","            break\n"],"metadata":{"id":"sPt0ck3R7RN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test Results"],"metadata":{"id":"7P46k-y2U6M7"}},{"cell_type":"code","source":["# Example usage:\n","\n","# Setup\n","game = TicTacToeK(BOARD_ROWS, BOARD_COLS, WIN_SIZE)\n","args = {'num_simulations': 25, 'batch_size': 64}\n","\n","# Create tester\n","tester = ModelTester(game, model, args)\n","\n","# Create test opponents\n","test_opponents = create_test_suite(game, model, args)\n","\n","# Run comprehensive tests\n","results = tester.comprehensive_test(test_opponents, num_games_per_opponent=100)\n","\n","# Generate report and visualizations\n","tester.create_performance_report()\n","tester.plot_results()\n","\n","# Analyze playing style\n","style_analysis = tester.analyze_playing_style(RandomPlayer(), num_games=50)\n","print(\"Playing style analysis:\", style_analysis)\n","\n","# Save results for later analysis\n","tester.save_results('my_model_results.json')"],"metadata":{"id":"fE3VhZBDUvFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## You can play with any model yourself."],"metadata":{"id":"Kbv1wI4qVCL4"}},{"cell_type":"code","source":["agent = # ...\n","play_against_agent(game, agent, human_first=True)"],"metadata":{"id":"R_oNXIN7VI2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### â­ Bonus Task (3 extra points)\n","\n","Implement an agent that **never loses** in the 3Ã—3 Connect-3 game:\n","- It should at least guarantee a draw against any opponent.\n","- Tip: You may hardcode logic, use exhaustive search, or simplify MCTS with perfect rollout.\n","\n","Once implemented, evaluate this agent on **larger boards** (e.g., 4Ã—4 or 5Ã—5) and reflect on its limitations in more complex settings."],"metadata":{"id":"Q_I-XxAsGc9W"}},{"cell_type":"code","source":[],"metadata":{"id":"zAFU0CrbGdQi"},"execution_count":null,"outputs":[]}]}